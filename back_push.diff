diff --git a/icu-makefiles/CMakeLists.txt b/icu-makefiles/CMakeLists.txt
index ecdc5c0..135db6e 100644
--- a/icu-makefiles/CMakeLists.txt
+++ b/icu-makefiles/CMakeLists.txt
@@ -485,12 +485,21 @@ ADD_DEPENDENCIES(icu_all icuuc icui18n icustubdata)
 
 
 install(DIRECTORY ${CMAKE_SOURCE_DIR}/${ICU_VERSION_DIR}/source/common/unicode/
-      DESTINATION include/unicode
-      PATTERN "*.h")
+      DESTINATION include/common/unicode
+      FILES_MATCHING PATTERN "*.h")
 
 install(DIRECTORY ${CMAKE_SOURCE_DIR}/${ICU_VERSION_DIR}/source/common/
-      DESTINATION include/
-      PATTERN "*.h")
+      DESTINATION include/common
+      FILES_MATCHING PATTERN "*.h")
+
+install(DIRECTORY ${CMAKE_SOURCE_DIR}/${ICU_VERSION_DIR}/source/i18n/unicode/
+      DESTINATION include/i18n/unicode
+      FILES_MATCHING PATTERN "*.h")
+
+install(DIRECTORY ${CMAKE_SOURCE_DIR}/${ICU_VERSION_DIR}/source/i18n/
+      DESTINATION include/i18n
+      FILES_MATCHING PATTERN "*.h")
 
 install(TARGETS icuuc icui18n icustubdata
         DESTINATION lib)
+
diff --git a/cos_diff.patch b/patch/cos_diff.patch
similarity index 100%
rename from cos_diff.patch
rename to patch/cos_diff.patch
diff --git a/devdeps-relaxed-rapidjson.diff b/patch/devdeps-relaxed-rapidjson.diff
similarity index 100%
rename from devdeps-relaxed-rapidjson.diff
rename to patch/devdeps-relaxed-rapidjson.diff
diff --git a/patch/hdfs.patch b/patch/hdfs.patch
new file mode 100644
index 0000000..4d390db
--- /dev/null
+++ b/patch/hdfs.patch
@@ -0,0 +1,988 @@
+
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/pom.xml b/hadoop-hdfs-project/hadoop-hdfs-native-client/pom.xml
+index 04cf060df4..e594c9f37b 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/pom.xml
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/pom.xml
+@@ -98,98 +98,6 @@ https://maven.apache.org/xsd/maven-4.0.0.xsd">
+     </plugins>
+   </build>
+   <profiles>
+-    <profile>
+-      <id>native-win</id>
+-      <activation>
+-        <activeByDefault>false</activeByDefault>
+-        <os>
+-          <family>windows</family>
+-        </os>
+-      </activation>
+-      <properties>
+-        <runningWithNative>true</runningWithNative>
+-      </properties>
+-      <build>
+-        <plugins>
+-          <plugin>
+-            <groupId>org.apache.maven.plugins</groupId>
+-            <artifactId>maven-enforcer-plugin</artifactId>
+-            <executions>
+-              <execution>
+-                <id>enforce-os</id>
+-                <goals>
+-                  <goal>enforce</goal>
+-                </goals>
+-                <configuration>
+-                  <rules>
+-                    <requireOS>
+-                      <family>windows</family>
+-                      <message>native-win build only supported on Windows</message>
+-                    </requireOS>
+-                  </rules>
+-                  <fail>true</fail>
+-                </configuration>
+-              </execution>
+-            </executions>
+-          </plugin>
+-          <plugin>
+-            <groupId>org.apache.maven.plugins</groupId>
+-            <artifactId>maven-antrun-plugin</artifactId>
+-            <executions>
+-              <execution>
+-                <id>make</id>
+-                <phase>compile</phase>
+-                <goals>
+-                  <goal>run</goal>
+-                </goals>
+-                <configuration>
+-                  <target>
+-                    <mkdir dir="${project.build.directory}/native"/>
+-                    <exec executable="cmake" dir="${project.build.directory}/native"
+-                          failonerror="true">
+-                      <arg line="${basedir}/src/ -DGENERATED_JAVAH=${project.build.directory}/native/javah -DJVM_ARCH_DATA_MODEL=${sun.arch.data.model} -DHADOOP_BUILD=1 -DREQUIRE_FUSE=${require.fuse} -DREQUIRE_VALGRIND=${require.valgrind} -A '${env.PLATFORM}'"/>
+-                      <arg line="${native_cmake_args}"/>
+-                    </exec>
+-                    <exec executable="msbuild" dir="${project.build.directory}/native"
+-                          failonerror="true">
+-                      <arg line="ALL_BUILD.vcxproj /nologo /p:Configuration=RelWithDebInfo /p:LinkIncremental=false"/>
+-                      <arg line="${native_make_args}"/>
+-                    </exec>
+-                    <!-- Copy for inclusion in distribution. -->
+-                    <copy todir="${project.build.directory}/bin">
+-                      <fileset dir="${project.build.directory}/native/bin/RelWithDebInfo"/>
+-                    </copy>
+-                  </target>
+-                </configuration>
+-              </execution>
+-              <execution>
+-                <id>native_tests</id>
+-                <phase>test</phase>
+-                <goals><goal>run</goal></goals>
+-                <configuration>
+-                  <skip>${skipTests}</skip>
+-                  <target>
+-                    <property name="compile_classpath" refid="maven.compile.classpath"/>
+-                    <property name="test_classpath" refid="maven.test.classpath"/>
+-                    <exec executable="ctest" failonerror="true" dir="${project.build.directory}/native">
+-                      <arg line="--output-on-failure"/>
+-                      <arg line="${native_ctest_args}"/>
+-                      <env key="CLASSPATH" value="${test_classpath}:${compile_classpath}"/>
+-                      <!-- HADOOP_HOME required to find winutils. -->
+-                      <env key="HADOOP_HOME" value="${hadoop.common.build.dir}"/>
+-                      <!-- Make sure hadoop.dll and jvm.dll are on PATH. -->
+-                      <env key="PATH" value="${env.PATH};${hadoop.common.build.dir}/bin;${java.home}/jre/bin/server;${java.home}/bin/server"/>
+-                      <!-- Make sure libhadoop.so is on LD_LIBRARY_PATH. -->
+-                      <env key="LD_LIBRARY_PATH" value="${env.LD_LIBRARY_PATH}:${project.build.directory}/native/target/usr/local/lib:${hadoop.common.build.dir}/native/target/usr/local/lib"/>
+-                    </exec>
+-                  </target>
+-                </configuration>
+-              </execution>
+-            </executions>
+-          </plugin>
+-        </plugins>
+-      </build>
+-    </profile>
+     <profile>
+       <id>native</id>
+       <activation>
+@@ -216,6 +124,8 @@ https://maven.apache.org/xsd/maven-4.0.0.xsd">
+                     <REQUIRE_FUSE>${require.fuse}</REQUIRE_FUSE>
+                     <REQUIRE_VALGRIND>${require.valgrind}</REQUIRE_VALGRIND>
+                     <HADOOP_BUILD>1</HADOOP_BUILD>
++                    <HDFSPP_LIBRARY_ONLY>1</HDFSPP_LIBRARY_ONLY>
++                    <GSASL_DIR>${env.GSASL_HOME}</GSASL_DIR>
+                     <REQUIRE_LIBWEBHDFS>${require.libwebhdfs}</REQUIRE_LIBWEBHDFS>
+                     <REQUIRE_OPENSSL>${require.openssl}</REQUIRE_OPENSSL>
+                     <CUSTOM_OPENSSL_PREFIX>${openssl.prefix}</CUSTOM_OPENSSL_PREFIX>
+@@ -257,73 +167,5 @@ https://maven.apache.org/xsd/maven-4.0.0.xsd">
+         </plugins>
+       </build>
+     </profile>
+-    <profile>
+-      <id>native-clang</id>
+-      <activation>
+-        <activeByDefault>false</activeByDefault>
+-      </activation>
+-      <properties>
+-        <runningWithNative>true</runningWithNative>
+-      </properties>
+-      <build>
+-        <plugins>
+-          <plugin>
+-            <groupId>org.apache.hadoop</groupId>
+-            <artifactId>hadoop-maven-plugins</artifactId>
+-            <executions>
+-              <execution>
+-                <id>cmake-compile-clang</id>
+-                <phase>compile</phase>
+-                <goals><goal>cmake-compile</goal></goals>
+-                <configuration>
+-                  <source>${basedir}/src</source>
+-                  <vars>
+-                    <CMAKE_C_COMPILER>clang</CMAKE_C_COMPILER>
+-                    <CMAKE_CXX_COMPILER>clang++</CMAKE_CXX_COMPILER>
+-                    <GENERATED_JAVAH>${project.build.directory}/native/javah</GENERATED_JAVAH>
+-                    <JVM_ARCH_DATA_MODEL>${sun.arch.data.model}</JVM_ARCH_DATA_MODEL>
+-                    <REQUIRE_FUSE>${require.fuse}</REQUIRE_FUSE>
+-                    <REQUIRE_VALGRIND>${require.valgrind}</REQUIRE_VALGRIND>
+-                    <HADOOP_BUILD>1</HADOOP_BUILD>
+-                    <REQUIRE_LIBWEBHDFS>${require.libwebhdfs}</REQUIRE_LIBWEBHDFS>
+-                    <REQUIRE_OPENSSL>${require.openssl}</REQUIRE_OPENSSL>
+-                    <CUSTOM_OPENSSL_PREFIX>${openssl.prefix}</CUSTOM_OPENSSL_PREFIX>
+-                    <CUSTOM_OPENSSL_LIB>${openssl.lib}</CUSTOM_OPENSSL_LIB>
+-                    <CUSTOM_OPENSSL_INCLUDE>${openssl.include}</CUSTOM_OPENSSL_INCLUDE>
+-                  </vars>
+-                  <output>${project.build.directory}/clang</output>
+-                </configuration>
+-              </execution>
+-            </executions>
+-          </plugin>
+-          <plugin>
+-            <groupId>org.apache.maven.plugins</groupId>
+-            <artifactId>maven-antrun-plugin</artifactId>
+-            <executions>
+-              <execution>
+-                <id>native_tests_clang</id>
+-                <phase>test</phase>
+-                <goals><goal>run</goal></goals>
+-                <configuration>
+-                  <skip>${skipTests}</skip>
+-                  <target>
+-                    <property name="compile_classpath" refid="maven.compile.classpath"/>
+-                    <property name="test_classpath" refid="maven.test.classpath"/>
+-                    <exec executable="ctest" failonerror="true" dir="${project.build.directory}/clang">
+-                      <arg line="--output-on-failure"/>
+-                      <arg line="${native_ctest_args}"/>
+-                      <env key="CLASSPATH" value="${test_classpath}:${compile_classpath}"/>
+-                      <!-- Make sure libhadoop.so is on LD_LIBRARY_PATH. -->
+-                      <env key="LD_LIBRARY_PATH" value="${env.LD_LIBRARY_PATH}:${project.build.directory}/clang/target/usr/local/lib:${hadoop.common.build.dir}/native/target/usr/local/lib"/>
+-                      <env key="DYLD_LIBRARY_PATH" value="${env.DYLD_LIBRARY_PATH}:${project.build.directory}/clang/target/usr/local/lib:${hadoop.common.build.dir}/native/target/usr/local/lib"/>
+-                    </exec>
+-                  </target>
+-                </configuration>
+-              </execution>
+-            </executions>
+-          </plugin>
+-        </plugins>
+-      </build>
+-    </profile>
+   </profiles>
+ </project>
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/CMakeLists.txt b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/CMakeLists.txt
+index a7fb311125..f08bb77d4d 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/CMakeLists.txt
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/CMakeLists.txt
+@@ -18,6 +18,16 @@
+ 
+ cmake_minimum_required(VERSION 3.1 FATAL_ERROR)
+ 
++if(UNIX)
++set (CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wall -Wextra -pedantic -std=c++11 -g -fPIC -fno-strict-aliasing")
++set (CMAKE_C_FLAGS "${CMAKE_C_FLAGS} -g -fPIC -fno-strict-aliasing")
++endif()
++
++if (CMAKE_CXX_COMPILER_ID STREQUAL "Clang")
++    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++11")
++    add_definitions(-DASIO_HAS_STD_ADDRESSOF -DASIO_HAS_STD_ARRAY -DASIO_HAS_STD_ATOMIC -DASIO_HAS_CSTDINT -DASIO_HAS_STD_SHARED_PTR -DASIO_HAS_STD_TYPE_TRAITS -DASIO_HAS_VARIADIC_TEMPLATES -DASIO_HAS_STD_FUNCTION -DASIO_HAS_STD_CHRONO -DASIO_HAS_STD_SYSTEM_ERROR)
++endif ()
++
+ add_definitions(-DLIBHDFS_DLL_EXPORT)
+ 
+ include_directories(
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.c b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.c
+index 4efb3b61b4..cbf57bdf52 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.c
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.c
+@@ -714,26 +714,26 @@ static JNIEnv* getGlobalJNIEnv(void)
+         }
+         options[0].optionString = optHadoopClassPath;
+         hadoopJvmArgs = getenv("LIBHDFS_OPTS");
+-	if (hadoopJvmArgs != NULL)  {
+-          hadoopJvmArgs = strdup(hadoopJvmArgs);
+-          for (noArgs = 1, str = hadoopJvmArgs; ; noArgs++, str = NULL) {
+-            token = strtok_r(str, jvmArgDelims, &savePtr);
+-            if (NULL == token) {
+-              break;
++        if (hadoopJvmArgs != NULL)  {
++            hadoopJvmArgs = strdup(hadoopJvmArgs);
++            for (noArgs = 1, str = hadoopJvmArgs; ; noArgs++, str = NULL) {
++                token = strtok_r(str, jvmArgDelims, &savePtr);
++                if (NULL == token) {
++                break;
++                }
++                options[noArgs].optionString = token;
+             }
+-            options[noArgs].optionString = token;
+-          }
+         }
+ 
+         //Create the VM
+-        vm_args.version = JNI_VERSION_1_2;
++        vm_args.version = JNI_VERSION_1_8;
+         vm_args.options = options;
+         vm_args.nOptions = noArgs; 
+         vm_args.ignoreUnrecognized = 1;
+ 
+         rv = JNI_CreateJavaVM(&vm, (void*)&env, &vm_args);
+ 
+-        if (hadoopJvmArgs != NULL)  {
++        if (hadoopJvmArgs != NULL) {
+           free(hadoopJvmArgs);
+         }
+         free(optHadoopClassPath);
+@@ -744,7 +744,6 @@ static JNIEnv* getGlobalJNIEnv(void)
+                     "with error: %d\n", rv);
+             return NULL;
+         }
+-
+         // We use findClassAndInvokeMethod here because the jclasses in
+         // jclasses.h have not loaded yet
+         jthr = findClassAndInvokeMethod(env, NULL, STATIC, NULL, HADOOP_FS,
+@@ -758,6 +757,8 @@ static JNIEnv* getGlobalJNIEnv(void)
+         //Attach this thread to the VM
+         vm = vmBuf[0];
+         rv = (*vm)->AttachCurrentThread(vm, (void*)&env, 0);
++        fprintf(stderr, "DEBUG Call to AttachCurrentThread "
++                    "failed with error: %d\n", rv);
+         if (rv != 0) {
+             fprintf(stderr, "Call to AttachCurrentThread "
+                     "failed with error: %d\n", rv);
+@@ -768,6 +769,125 @@ static JNIEnv* getGlobalJNIEnv(void)
+     return env;
+ }
+ 
++static jint destroyGlobalJNIEnv(void)
++{
++    JavaVM* vmBuf[VM_BUF_LENGTH]; 
++    jint rv = 0; 
++    jint noVMs = 0;
++    JavaVM *vm;
++
++    rv = JNI_GetCreatedJavaVMs(&(vmBuf[0]), VM_BUF_LENGTH, &noVMs);
++    if (rv != 0) {
++        fprintf(stderr, "DEBUG destroyGlobalJNIEnv: JNI_GetCreatedJavaVMs failed with error: %d\n", rv);
++        return rv;
++    }
++    if (noVMs == 0) {
++        // do nothing
++        fprintf(stdout, "DEBUG destroyGlobalJNIEnv without exists jvm\n");
++        return 0;
++    }
++
++    if (noVMs != 1) {
++        fprintf(stderr, "DEBUG destroyGlobalJNIEnv with unexpected jvm count: %d\n", noVMs);
++        return 1;
++    } else {
++        fprintf(stderr, "DEBUG destroyGlobalJNIEnv with one jvm should destroy\n");
++        vm = vmBuf[0];
++        if (vm) {
++            rv = (*vm)->DestroyJavaVM(vm);
++            if (0 != rv) {
++                fprintf(stderr, "DEBUG destroyGlobalJNIEnv failed with error: %d\n", rv);
++                return rv;
++            }
++        } else {
++            fprintf(stderr, "DEBUG destroyGlobalJNIEnv with null vm\n");
++            return 2;
++        }
++    }
++
++    fprintf(stderr, "DEBUG destroyGlobalJNIEnv: make sure vm is cleaned\n");
++    rv = JNI_GetCreatedJavaVMs(&(vmBuf[0]), VM_BUF_LENGTH, &noVMs);
++    if (0 != rv) {
++      fprintf(stdout,
++              "DEBUG destroyGlobalJNIEnv: JNI_GetCreatedJavaVMs check failed with error: %d\n",
++              rv);
++      return rv;
++    }
++    if (0 != noVMs) {
++      fprintf(stderr,
++              "DEBUG destroyGlobalJNIEnv: JNI_GetCreatedJavaVMs check unexpected count vms: %d\n",
++              noVMs);
++      return 3;
++    }
++
++    fprintf(stderr, "DEBUG destroyGlobalJNIEnv: JNI_GetCreatedJavaVMs check success\n");
++    return 0;
++}
++
++jint detachCurrentThread(void)
++{
++    struct ThreadLocalState *state = NULL;
++    THREAD_LOCAL_STORAGE_GET_QUICK(&state);
++    mutexLock(&jvmMutex);
++    if (state) {
++        hdfsThreadDestructor(state);
++        THREAD_LOCAL_STORAGE_SET_QUICK(NULL);
++        if (threadLocalStorageSet(NULL)) {
++            fprintf(stderr, "DEBUG detachCurrentThread: threadLocalStorageSet null failed\n");
++        } else {
++            fprintf(stderr, "DEBUG detachCurrentThread: threadLocalStorageSet null success\n");
++        }
++        mutexUnlock(&jvmMutex);
++    } else if (threadLocalStorageGet(&state)) {
++      hdfsThreadDestructor(state);
++      THREAD_LOCAL_STORAGE_SET_QUICK(NULL);
++      if (threadLocalStorageSet(NULL)) {
++        fprintf(stderr, "DEBUG detachCurrentThread: threadLocalStorageGet and threadLocalStorageSet failed\n");
++      } else {
++        fprintf(stderr, "DEBUG detachCurrentThread: threadLocalStorageGet and threadLocalStorageSet success\n");
++      }
++      mutexUnlock(&jvmMutex);
++    } else if (state) {
++      // Free any stale exception strings.
++      free(state->lastExceptionRootCause);
++      free(state->lastExceptionStackTrace);
++      state->lastExceptionRootCause = NULL;
++      state->lastExceptionStackTrace = NULL;
++      hdfsThreadDestructor(state);
++      THREAD_LOCAL_STORAGE_SET_QUICK(NULL);
++      if (threadLocalStorageSet(NULL)) {
++        fprintf(stderr, "DEBUG detachCurrentThread: state and threadLocalStorageSet null failed\n");
++      } else {
++        fprintf(stderr, "DEBUG detachCurrentThread: state and threadLocalStorageSet null success\n");
++      }
++      mutexUnlock(&jvmMutex);
++    } else {
++      THREAD_LOCAL_STORAGE_SET_QUICK(NULL);
++      if (threadLocalStorageSet(NULL)) {
++        fprintf(stderr, "DEBUG detachCurrentThread: last set null failed\n");
++      } else {
++        fprintf(stderr, "DEBUG detachCurrentThread: last set null success\n");
++      }
++      mutexUnlock(&jvmMutex);
++    }
++    return 0;
++}
++
++jint destroyJNIEnv(void)
++{
++    mutexLock(&jvmMutex);
++    fprintf(stderr, "DEBUG destroyJNIEnv: destory global jni env\n");
++    jint rv = 0;
++    rv = destroyGlobalJNIEnv();
++    if (0 != rv) {
++        fprintf(stderr, "DEBUG destroyJNIEnv: destroyGlobalJNIEnv failed with error: %d\n", rv);
++    } else {
++        fprintf(stderr, "DEBUG destroyJNIEnv: destroyGlobalJNIEnv success\n");
++    }
++    mutexUnlock(&jvmMutex);
++    return rv;
++}
++
+ /**
+  * getJNIEnv: A helper function to get the JNIEnv* for the given thread.
+  * If no JVM exists, then one will be created. JVM command line arguments
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.h b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.h
+index 41d6fab2a7..f4a6036ca1 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.h
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.h
+@@ -36,6 +36,31 @@
+     #define PATH_SEPARATOR_STR ":"
+ #endif
+ 
++/**
++ *  Note: The #defines below are copied directly from libhdfs'
++ *  hdfs.h.  LIBHDFS_EXTERNAL gets explicitly #undefed at the
++ *  end of the file so it must be redefined here.
++ **/
++
++#ifdef WIN32
++    #ifdef LIBHDFS_DLL_EXPORT
++        #define LIBHDFS_EXTERNAL __declspec(dllexport)
++    #elif LIBHDFS_DLL_IMPORT
++        #define LIBHDFS_EXTERNAL __declspec(dllimport)
++    #else
++        #define LIBHDFS_EXTERNAL
++    #endif
++#else
++    #ifdef LIBHDFS_DLL_EXPORT
++        #define LIBHDFS_EXTERNAL __attribute__((visibility("default")))
++    #elif LIBHDFS_DLL_IMPORT
++        #define LIBHDFS_EXTERNAL __attribute__((visibility("default")))
++    #else
++        #define LIBHDFS_EXTERNAL
++    #endif
++#endif
++
++
+ // #define _LIBHDFS_JNI_HELPER_DEBUGGING_ON_
+ 
+ /** Denote the method we want to invoke as STATIC or INSTANCE */
+@@ -129,6 +154,30 @@ jthrowable methodIdFromClass(jclass cls, const char *className,
+  */
+ jthrowable classNameOfObject(jobject jobj, JNIEnv *env, char **name);
+ 
++/** detachCurrentThread: A helper function to detach the JNIEnv* for the given thread.
++ * It gets this from the ThreadLocalState if it exists. If a ThreadLocalState
++ * does not exist, one will be created in getJNIEnv.
++ * If no JVM exists, then method quits directly and return zero. 
++ * If not, then method will detach state and return zero if detach succeds.
++ * If return non-zero that means detach failed.
++ * @param: None.
++ * @return The status code corresponding to destroy the thread.
++ * */
++LIBHDFS_EXTERNAL
++jint detachCurrentThread(void);
++
++/** destroyJNIEnv: A helper function to destroy the JNIEnv* for the given thread.
++ * It gets this from the ThreadLocalState if it exists. If a ThreadLocalState
++ * does not exist, one will be created in getJNIEnv.
++ * If no JVM exists, then method quits directly and return zero. 
++ * If not, then method will destory state and return zero if destory succeds.
++ * If return non-zero that means destroy failed.
++ * @param: None.
++ * @return The status code corresponding to destroy the thread.
++ * */
++LIBHDFS_EXTERNAL
++jint destroyJNIEnv(void);
++
+ /** getJNIEnv: A helper function to get the JNIEnv* for the given thread.
+  * It gets this from the ThreadLocalState if it exists. If a ThreadLocalState
+  * does not exist, one will be created.
+@@ -137,6 +186,7 @@ jthrowable classNameOfObject(jobject jobj, JNIEnv *env, char **name);
+  * @param: None.
+  * @return The JNIEnv* corresponding to the thread.
+  * */
++LIBHDFS_EXTERNAL
+ JNIEnv* getJNIEnv(void);
+ 
+ /**
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/os/thread_local_storage.h b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/os/thread_local_storage.h
+index 025ceff148..334c92a430 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/os/thread_local_storage.h
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/os/thread_local_storage.h
+@@ -33,9 +33,10 @@
+  * is initialized by the linker.  The following macros use this technique on the
+  * operating systems that support it.
+  */
++static __thread struct ThreadLocalState *quickTlsEnv = NULL;
++
+ #ifdef HAVE_BETTER_TLS
+   #define THREAD_LOCAL_STORAGE_GET_QUICK(state) \
+-    static __thread struct ThreadLocalState *quickTlsEnv = NULL; \
+     { \
+       if (quickTlsEnv) { \
+         *state = quickTlsEnv; \
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/CMakeLists.txt b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/CMakeLists.txt
+index f4dd492234..d0b9facb5c 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/CMakeLists.txt
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/CMakeLists.txt
+@@ -31,11 +31,12 @@ cmake_minimum_required(VERSION 2.8)
+ enable_testing()
+ include (CTest)
+ 
+-SET(BUILD_SHARED_HDFSPP TRUE CACHE STRING "BUILD_SHARED_HDFSPP defaulting to 'TRUE'")
++SET(BUILD_SHARED_HDFSPP FALSE CACHE STRING "BUILD_SHARED_HDFSPP defaulting to 'FALSE'")
+ SET(CMAKE_MODULE_PATH "${CMAKE_CURRENT_SOURCE_DIR}/CMake" ${CMAKE_MODULE_PATH})
+ 
+ # If there's a better way to inform FindCyrusSASL.cmake, let's make this cleaner:
+ SET(CMAKE_PREFIX_PATH "${CMAKE_PREFIX_PATH};${CYRUS_SASL_DIR};${GSASL_DIR};$ENV{PROTOBUF_HOME}")
++# SET(CMAKE_PREFIX_PATH "${CMAKE_PREFIX_PATH};${CYRUS_SASL_DIR};${GSASL_DIR};${KRB5_HOME};$ENV{PROTOBUF_HOME}")
+ 
+ # Specify PROTOBUF_HOME so that find_package picks up the correct version
+ SET(CMAKE_PREFIX_PATH "${CMAKE_PREFIX_PATH};$ENV{PROTOBUF_HOME}")
+
+diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
+index 6caf1e7167..bc345010a3 100644
+--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
++++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java
+@@ -582,6 +582,19 @@ public FileSystem run() throws IOException {
+     });
+   }
+ 
++  public synchronized static FileSystem newInstanceFromKeytab(final URI uri, final Configuration conf,
++      final String principal, final String keytabPath)
++      throws IOException, InterruptedException {
++    UserGroupInformation.setConfiguration(conf);
++    UserGroupInformation.loginUserFromKeytab(principal, keytabPath);
++    return UserGroupInformation.getLoginUser().doAs(new PrivilegedExceptionAction<FileSystem>() {
++      @Override
++      public FileSystem run() throws IOException {
++        return newInstance(uri, conf);
++      }
++    });
++  }
++
+   /**
+    * Returns the FileSystem for this URI's scheme and authority.
+    * The entire URI is passed to the FileSystem instance's initialize method.
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs-examples/libhdfs_read.c b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs-examples/libhdfs_read.c
+index 419be1268b..fc9a07c4ca 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs-examples/libhdfs_read.c
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs-examples/libhdfs_read.c
+@@ -61,7 +61,7 @@ int main(int argc, char **argv) {
+     curSize = bufferSize;
+     for (; curSize == bufferSize;) {
+         curSize = hdfsRead(fs, readFile, (void*)buffer, curSize);
+-    }
++        }
+ 
+ 
+     free(buffer);
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/exception.c b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/exception.c
+index fec9a103b4..ed61b77fca 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/exception.c
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/exception.c
+@@ -90,6 +90,16 @@ static const struct ExceptionInfo gExceptionInfo[] = {
+         0,
+         ESTALE,
+     },
++    {
++        "org.apache.hadoop.security.KerberosAuthException",
++        NOPRINT_EXC_ACCESS_CONTROL,
++        ETIME
++    },
++    {
++        "java.net.ConnectException",
++        0,
++        EFAULT
++    },
+ };
+ 
+ void getExceptionInfo(const char *excName, int noPrintFlags,
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/hdfs.c b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/hdfs.c
+index ed150925cd..b3ba27f867 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/hdfs.c
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/hdfs.c
+@@ -25,6 +25,7 @@
+ #include <fcntl.h>
+ #include <inttypes.h>
+ #include <stdio.h>
++#include <stdlib.h>
+ #include <string.h>
+ 
+ #define JAVA_VOID       "V"
+@@ -35,6 +36,7 @@
+ #define JMETHOD1(X, R)      "(" X ")" R
+ #define JMETHOD2(X, Y, R)   "(" X Y ")" R
+ #define JMETHOD3(X, Y, Z, R)   "(" X Y Z")" R
++#define JMETHOD4(X, Y, Z, A, R)   "(" X Y Z A")" R
+ 
+ #define KERBEROS_TICKET_CACHE_PATH "hadoop.security.kerberos.ticket.cache.path"
+ 
+@@ -495,6 +497,11 @@ done:
+     return ret;
+ }
+ 
++struct hdfsBuilderConfFileOpt {
++    struct hdfsBuilderConfFileOpt *next;
++    const char *currentPath;
++};
++
+ struct hdfsBuilderConfOpt {
+     struct hdfsBuilderConfOpt *next;
+     const char *key;
+@@ -506,10 +513,27 @@ struct hdfsBuilder {
+     const char *nn;
+     tPort port;
+     const char *kerbTicketCachePath;
++    const char *kerb5ConfPath;
++    const char *keyTabFile;
++    const char *kerbPrincipal;
+     const char *userName;
+     struct hdfsBuilderConfOpt *opts;
++    struct hdfsBuilderConfFileOpt *fileOpts;
+ };
+ 
++void hdfsBuilderSetKerb5Conf(struct hdfsBuilder *bld, const char *kerb5ConfPath) {
++    bld->kerb5ConfPath = kerb5ConfPath;
++    if (bld->kerb5ConfPath) {
++        systemPropertySetStr("java.security.krb5.conf", bld->kerb5ConfPath);
++        hdfsBuilderConfSetStr(bld, "hadoop.security.authorization", "true");
++        hdfsBuilderConfSetStr(bld, "hadoop.security.authentication", "kerberos");
++    }
++}
++
++void hdfsBuilderSetKeyTabFile(struct hdfsBuilder *bld, const char *keyTabFile) {
++    bld->keyTabFile = keyTabFile;
++}
++
+ struct hdfsBuilder *hdfsNewBuilder(void)
+ {
+     struct hdfsBuilder *bld = calloc(1, sizeof(struct hdfsBuilder));
+@@ -520,6 +544,42 @@ struct hdfsBuilder *hdfsNewBuilder(void)
+     return bld;
+ }
+ 
++int systemPropertySetStr(const char *key, const char *val) {
++    JNIEnv *env = 0;
++    jobject jRet = NULL;
++    jvalue  jVal;
++    jthrowable jthr;
++    int ret = EINTERNAL;
++    jstring jkey = NULL, jvalue0 = NULL;
++    env = getJNIEnv();
++    if (env == NULL) {
++        ret = EINTERNAL;
++        return ret;
++    }
++    jthr = newJavaStr(env, key, &jkey);
++    if (jthr) {
++        ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,"System::setProperty::new String");
++        goto done;
++    }
++    jthr = newJavaStr(env, val, &jvalue0);
++    if (jthr) {
++        ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,"System::setProperty::new String");
++        goto done;
++    }
++    jthr = invokeMethod(env, &jVal, STATIC, NULL, JC_SYSTEM, "setProperty", "(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;", jkey, jvalue0);
++    jRet = jVal.l;
++    ret = 0;
++    if (jthr) {
++        ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,"System::setProperty");
++        goto done;
++    }
++    done:
++        destroyLocalReference(env, jRet);
++        destroyLocalReference(env, jkey);
++        destroyLocalReference(env, jvalue0);
++        return ret;
++}
++
+ int hdfsBuilderConfSetStr(struct hdfsBuilder *bld, const char *key,
+                           const char *val)
+ {
+@@ -546,6 +606,13 @@ void hdfsFreeBuilder(struct hdfsBuilder *bld)
+         free(cur);
+         cur = next;
+     }
++    struct hdfsBuilderConfFileOpt *cur0, *next0;
++    cur0 = bld->fileOpts;
++    for (cur0 = bld->fileOpts; cur0;) {
++        next0 = cur0->next;
++        free(cur0);
++        cur0 = next0;
++    }
+     free(bld);
+ }
+ 
+@@ -569,6 +636,11 @@ void hdfsBuilderSetUserName(struct hdfsBuilder *bld, const char *userName)
+     bld->userName = userName;
+ }
+ 
++void hdfsBuilderSetPrincipal(struct hdfsBuilder *bld, const char *kerbPrincipal)
++{
++    bld->kerbPrincipal = kerbPrincipal;
++}
++
+ void hdfsBuilderSetKerbTicketCachePath(struct hdfsBuilder *bld,
+                                        const char *kerbTicketCachePath)
+ {
+@@ -688,17 +760,32 @@ static const char *hdfsBuilderToStr(const struct hdfsBuilder *bld,
+     return buf;
+ }
+ 
++jthrowable hadoopConfFileAdd(JNIEnv *env, jobject jConfiguration, const char *path) {
++    /* Create an object of org.apache.hadoop.fs.Path */
++    jobject jPath = NULL;
++    jthrowable jthr;
++    jthr = constructNewObjectOfPath(env, path, &jPath);
++    if (jthr) {
++        goto done;
++    }
++    jthr = invokeMethod(env, NULL, INSTANCE, jConfiguration, JC_CONFIGURATION, "addResource", JMETHOD1(JPARAM(HADOOP_PATH), JAVA_VOID), jPath);
++    done:
++    destroyLocalReference(env, jPath);
++    return jthr;
++}
++
+ hdfsFS hdfsBuilderConnect(struct hdfsBuilder *bld)
+ {
+     JNIEnv *env = 0;
+     jobject jConfiguration = NULL, jFS = NULL, jURI = NULL, jCachePath = NULL;
+-    jstring jURIString = NULL, jUserString = NULL;
++    jstring jURIString = NULL, jUserString = NULL, jPrincipalString = NULL, jKeyTabString = NULL;
+     jvalue  jVal;
+     jthrowable jthr = NULL;
+     char *cURI = 0, buf[512];
+     int ret;
+     jobject jRet = NULL;
+     struct hdfsBuilderConfOpt *opt;
++    struct hdfsBuilderConfFileOpt *fileOpt;
+ 
+     //Get the JNIEnv* corresponding to current thread
+     env = getJNIEnv();
+@@ -715,6 +802,18 @@ hdfsFS hdfsBuilderConnect(struct hdfsBuilder *bld)
+             "hdfsBuilderConnect(%s)", hdfsBuilderToStr(bld, buf, sizeof(buf)));
+         goto done;
+     }
++
++    for (fileOpt = bld->fileOpts; fileOpt; fileOpt = fileOpt->next) {
++        // conf.addResource(new Path(fileOpt->path))
++        jthr = hadoopConfFileAdd(env, jConfiguration, fileOpt->currentPath);
++        if (jthr) {
++            ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,
++                                        "hdfsBuilderConnect(%s): error adding conffile '%s'",
++                                        hdfsBuilderToStr(bld, buf, sizeof(buf)), fileOpt->currentPath);
++            goto done;
++        }
++    }
++
+     // set configuration values
+     for (opt = bld->opts; opt; opt = opt->next) {
+         jthr = hadoopConfSetStr(env, jConfiguration, opt->key, opt->val);
+@@ -746,7 +845,7 @@ hdfsFS hdfsBuilderConnect(struct hdfsBuilder *bld)
+             // fs = FileSytem#getLocal(conf);
+             jthr = invokeMethod(env, &jVal, STATIC, NULL,
+                     JC_FILE_SYSTEM, "getLocal",
+-                    JMETHOD1(JPARAM(HADOOP_CONF), JPARAM(HADOOP_LOCALFS)),
++                    JMETHOD1(JPARAM(HADOOP_CONF)    , JPARAM(HADOOP_LOCALFS)),
+                     jConfiguration);
+             if (jthr) {
+                 ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,
+@@ -794,6 +893,13 @@ hdfsFS hdfsBuilderConnect(struct hdfsBuilder *bld)
+             jURI = jVal.l;
+         }
+ 
++        jthr = newJavaStr(env, bld->userName, &jUserString);
++        if (jthr) {
++            ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,
++                                        "hdfsBuilderConnect(%s)",
++                                        hdfsBuilderToStr(bld, buf, sizeof(buf)));
++            goto done;
++        }
+         if (bld->kerbTicketCachePath) {
+             jthr = hadoopConfSetStr(env, jConfiguration,
+                 KERBEROS_TICKET_CACHE_PATH, bld->kerbTicketCachePath);
+@@ -803,33 +909,81 @@ hdfsFS hdfsBuilderConnect(struct hdfsBuilder *bld)
+                     hdfsBuilderToStr(bld, buf, sizeof(buf)));
+                 goto done;
+             }
+-        }
+-        jthr = newJavaStr(env, bld->userName, &jUserString);
+-        if (jthr) {
+-            ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,
+-                "hdfsBuilderConnect(%s)",
+-                hdfsBuilderToStr(bld, buf, sizeof(buf)));
+-            goto done;
++            invokeMethod(env, &jVal, STATIC, NULL, JC_SECURITY_CONFIGURATION, "setConfiguration",
++            JMETHOD1(JPARAM(HADOOP_CONF),JAVA_VOID), jConfiguration);
+         }
+         if (bld->forceNewInstance) {
+-            jthr = invokeMethod(env, &jVal, STATIC, NULL,
+-                    JC_FILE_SYSTEM, "newInstance",
+-                    JMETHOD3(JPARAM(JAVA_NET_URI), JPARAM(HADOOP_CONF),
+-                             JPARAM(JAVA_STRING), JPARAM(HADOOP_FS)), jURI,
+-                    jConfiguration, jUserString);
+-            if (jthr) {
+-                ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,
+-                    "hdfsBuilderConnect(%s)",
+-                    hdfsBuilderToStr(bld, buf, sizeof(buf)));
+-                goto done;
++            // need kerb5ConfPath to enable kerberos authentication
++            if (bld->kerb5ConfPath && bld->kerbPrincipal && bld->keyTabFile) {
++                jthr = newJavaStr(env, bld->kerbPrincipal, &jPrincipalString);
++                if (jthr) {
++                    ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,
++                                                "hdfsBuilderConnect(%s)",
++                                                hdfsBuilderToStr(bld, buf, sizeof(buf)));
++                    goto done;
++                }
++                jthr = newJavaStr(env, bld->keyTabFile, &jKeyTabString);
++                if (jthr) {
++                    ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,"hdfsBuilderConnect(%s)", hdfsBuilderToStr(bld, buf, sizeof(buf)));
++                    goto done;
++                }
++                jthr = invokeMethod(env, &jVal, STATIC, NULL,
++                                    JC_FILE_SYSTEM, "newInstanceFromKeytab",
++                                    JMETHOD4(JPARAM(JAVA_NET_URI), JPARAM(HADOOP_CONF),
++                                             JPARAM(JAVA_STRING), JPARAM(JAVA_STRING), JPARAM(HADOOP_FS)), jURI,
++                                    jConfiguration, jPrincipalString, jKeyTabString);
++                if (jthr) {
++                    ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,
++                                                "hdfsBuilderConnect(%s)",
++                                                hdfsBuilderToStr(bld, buf, sizeof(buf)));
++                    goto done;
++                }
++            } else {
++                jthr = invokeMethod(env, &jVal, STATIC, NULL,
++                                    JC_FILE_SYSTEM, "newInstance",
++                                    JMETHOD3(JPARAM(JAVA_NET_URI), JPARAM(HADOOP_CONF),
++                                             JPARAM(JAVA_STRING), JPARAM(HADOOP_FS)), jURI,
++                                    jConfiguration, jUserString);
++                if (jthr) {
++                    ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,
++                                                "hdfsBuilderConnect(%s)",
++                                                hdfsBuilderToStr(bld, buf, sizeof(buf)));
++                    goto done;
++                }
+             }
+             jFS = jVal.l;
+         } else {
+-            jthr = invokeMethod(env, &jVal, STATIC, NULL,
+-                    JC_FILE_SYSTEM, "get",
+-                    JMETHOD3(JPARAM(JAVA_NET_URI), JPARAM(HADOOP_CONF),
+-                            JPARAM(JAVA_STRING), JPARAM(HADOOP_FS)), jURI,
+-                            jConfiguration, jUserString);
++            if (bld->keyTabFile && bld->kerb5ConfPath && bld->kerbPrincipal) {
++                jthr = newJavaStr(env, bld->kerbPrincipal, &jPrincipalString);
++                if (jthr) {
++                    ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,
++                                                "hdfsBuilderConnect(%s)",
++                                                hdfsBuilderToStr(bld, buf, sizeof(buf)));
++                    goto done;
++                }
++                jthr = invokeMethod(env, NULL, STATIC, NULL, JC_SECURITY_CONFIGURATION, "setConfiguration", JMETHOD1(JPARAM(HADOOP_CONF),JAVA_VOID), jConfiguration);
++                if (jthr) {
++                    ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,"hdfsBuilderConnect(%s)", hdfsBuilderToStr(bld, buf, sizeof(buf)));
++                    goto done;
++                }
++                jthr = newJavaStr(env, bld->keyTabFile, &jKeyTabString);
++                if (jthr) {
++                    ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,"hdfsBuilderConnect(%s)", hdfsBuilderToStr(bld, buf, sizeof(buf)));
++                    goto done;
++                }
++                jthr = invokeMethod(env, NULL, STATIC, NULL, JC_SECURITY_CONFIGURATION, "loginUserFromKeytab", JMETHOD2(JPARAM(JAVA_STRING), JPARAM(JAVA_STRING), JAVA_VOID), jPrincipalString, jKeyTabString);
++                if (jthr) {
++                    ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,"hdfsBuilderConnect(%s)", hdfsBuilderToStr(bld, buf, sizeof(buf)));
++                    goto done;
++                }
++                jthr = invokeMethod(env, &jVal, STATIC, NULL, JC_FILE_SYSTEM, "get", JMETHOD1(JPARAM(HADOOP_CONF),
++                JPARAM(HADOOP_FS)), jConfiguration);
++            } else {
++                jthr = invokeMethod(env, &jVal, STATIC, NULL, JC_FILE_SYSTEM, "get",
++                                    JMETHOD3(JPARAM(JAVA_NET_URI), JPARAM(HADOOP_CONF),
++                                             JPARAM(JAVA_STRING), JPARAM(HADOOP_FS)),
++                                    jURI, jConfiguration, jUserString);
++            }
+             if (jthr) {
+                 ret = printExceptionAndFree(env, jthr, PRINT_EXC_ALL,
+                     "hdfsBuilderConnect(%s)",
+@@ -856,6 +1010,7 @@ done:
+     destroyLocalReference(env, jCachePath);
+     destroyLocalReference(env, jURIString);
+     destroyLocalReference(env, jUserString);
++    destroyLocalReference(env, jKeyTabString);
+     free(cURI);
+     hdfsFreeBuilder(bld);
+ 
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/include/hdfs/hdfs.h b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/include/hdfs/hdfs.h
+index eba50ff6eb..f88545f3ba 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/include/hdfs/hdfs.h
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/include/hdfs/hdfs.h
+@@ -294,6 +294,9 @@ extern  "C" {
+     LIBHDFS_EXTERNAL
+     struct hdfsBuilder *hdfsNewBuilder(void);
+ 
++    LIBHDFS_EXTERNAL
++    int systemPropertySetStr(const char *key, const char *val);
++
+     /**
+      * Force the builder to always create a new instance of the FileSystem,
+      * rather than possibly finding one in the cache.
+@@ -344,6 +347,15 @@ extern  "C" {
+      */
+     LIBHDFS_EXTERNAL
+     void hdfsBuilderSetUserName(struct hdfsBuilder *bld, const char *userName);
++    
++    /**
++    * Set the principal to use when connecting to the HDFS cluster.
++    *
++    * @param bld The HDFS builder
++    * @param kerbPrincipal The principal.  The string will be shallow-copied.
++    */
++    LIBHDFS_EXTERNAL
++    void hdfsBuilderSetPrincipal(struct hdfsBuilder *bld, const char *kerbPrincipal);
+ 
+     /**
+      * Set the path to the Kerberos ticket cache to use when connecting to
+@@ -357,6 +369,12 @@ extern  "C" {
+     void hdfsBuilderSetKerbTicketCachePath(struct hdfsBuilder *bld,
+                                    const char *kerbTicketCachePath);
+ 
++    LIBHDFS_EXTERNAL
++    void hdfsBuilderSetKerb5Conf(struct hdfsBuilder *bld, const char *kerb5ConfPath);
++
++    LIBHDFS_EXTERNAL
++    void hdfsBuilderSetKeyTabFile(struct hdfsBuilder *bld, const char *keyTabFile);
++
+     /**
+      * Free an HDFS builder.
+      *
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jclasses.c b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jclasses.c
+index 9f589ac257..44acb08d27 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jclasses.c
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jclasses.c
+@@ -112,6 +112,10 @@ jthrowable initCachedClasses(JNIEnv* env) {
+                 "org/apache/commons/lang3/exception/ExceptionUtils";
+         cachedJavaClasses[JC_CFUTURE].className =
+                 "java/util/concurrent/CompletableFuture";
++        cachedJavaClasses[JC_SYSTEM].className =
++                "java/lang/System";
++        cachedJavaClasses[JC_SECURITY_CONFIGURATION].className =
++                "org/apache/hadoop/security/UserGroupInformation";
+ 
+         // Create and set the jclass objects based on the class names set above
+         jthrowable jthr;
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jclasses.h b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jclasses.h
+index 0b174e1fec..4f28ef2c02 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jclasses.h
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jclasses.h
+@@ -61,6 +61,8 @@ typedef enum {
+     JC_ENUM_SET,
+     JC_EXCEPTION_UTILS,
+     JC_CFUTURE,
++    JC_SYSTEM,
++    JC_SECURITY_CONFIGURATION,
+     // A special marker enum that counts the number of cached jclasses
+     NUM_CACHED_CLASSES
+ } CachedJavaClass;
+@@ -101,6 +103,7 @@ const char *getClassName(CachedJavaClass cachedJavaClass);
+ #define HADOOP_FS_BLDR  "org/apache/hadoop/fs/FSBuilder"
+ #define HADOOP_RO       "org/apache/hadoop/fs/ReadOption"
+ #define HADOOP_DS       "org/apache/hadoop/net/unix/DomainSocket"
++#define HADOOP_USER_INFORMATION "org/apache/hadoop/security/UserGroupInformation"
+ 
+ /* Some frequently used Java class names */
+ #define JAVA_NET_ISA    "java/net/InetSocketAddress"
+@@ -111,6 +114,7 @@ const char *getClassName(CachedJavaClass cachedJavaClass);
+ #define JAVA_CFUTURE    "java/util/concurrent/CompletableFuture"
+ #define JAVA_TIMEUNIT   "java/util/concurrent/TimeUnit"
+ #define JAVA_OBJECT     "java/lang/Object"
++#define JAVA_SYSTEM     "java/lang/System"
+ 
+ /* Some frequently used third-party class names */
+ 
+diff --git a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.c b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.c
+index cbf57bdf52..b0da655984 100644
+--- a/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.c
++++ b/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.c
+@@ -237,7 +237,7 @@ jthrowable invokeMethod(JNIEnv *env, jvalue *retval, MethType methType,
+ static jthrowable constructNewObjectOfJclass(JNIEnv *env,
+         jobject *out, jclass cls, const char *className,
+                 const char *ctorSignature, va_list args) {
+-    jmethodID mid;
++    jmethodID mid = 0;
+     jobject jobj;
+     jthrowable jthr;
+ 
+@@ -655,7 +655,7 @@ static char* getClassPath()
+ static JNIEnv* getGlobalJNIEnv(void)
+ {
+     JavaVM* vmBuf[VM_BUF_LENGTH]; 
+-    JNIEnv *env;
++    JNIEnv *env = NULL;
+     jint rv = 0; 
+     jint noVMs = 0;
+     jthrowable jthr;
+-- 
+2.17.0
diff --git a/s2geometry-0.9.0-uint64.patch b/patch/s2geometry-0.9.0-uint64.patch
similarity index 100%
rename from s2geometry-0.9.0-uint64.patch
rename to patch/s2geometry-0.9.0-uint64.patch
diff --git a/patch/settings.xml b/patch/settings.xml
new file mode 100644
index 0000000..11f3a34
--- /dev/null
+++ b/patch/settings.xml
@@ -0,0 +1,271 @@
+<?xml version="1.0" encoding="UTF-8"?>
+
+<!--
+Licensed to the Apache Software Foundation (ASF) under one
+or more contributor license agreements.  See the NOTICE file
+distributed with this work for additional information
+regarding copyright ownership.  The ASF licenses this file
+to you under the Apache License, Version 2.0 (the
+"License"); you may not use this file except in compliance
+with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing,
+software distributed under the License is distributed on an
+"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+KIND, either express or implied.  See the License for the
+specific language governing permissions and limitations
+under the License.
+-->
+
+<!--
+ | This is the configuration file for Maven. It can be specified at two levels:
+ |
+ |  1. User Level. This settings.xml file provides configuration for a single user,
+ |                 and is normally provided in ${user.home}/.m2/settings.xml.
+ |
+ |                 NOTE: This location can be overridden with the CLI option:
+ |
+ |                 -s /path/to/user/settings.xml
+ |
+ |  2. Global Level. This settings.xml file provides configuration for all Maven
+ |                 users on a machine (assuming they're all using the same Maven
+ |                 installation). It's normally provided in
+ |                 ${maven.conf}/settings.xml.
+ |
+ |                 NOTE: This location can be overridden with the CLI option:
+ |
+ |                 -gs /path/to/global/settings.xml
+ |
+ | The sections in this sample file are intended to give you a running start at
+ | getting the most out of your Maven installation. Where appropriate, the default
+ | values (values used when the setting is not specified) are provided.
+ |
+ |-->
+<settings xmlns="http://maven.apache.org/SETTINGS/1.2.0"
+          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+          xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.2.0 https://maven.apache.org/xsd/settings-1.2.0.xsd">
+  <!-- localRepository
+   | The path to the local repository maven will use to store artifacts.
+   |
+   | Default: ${user.home}/.m2/repository
+  <localRepository>/path/to/local/repo</localRepository>
+  -->
+
+  <!-- interactiveMode
+   | This will determine whether maven prompts you when it needs input. If set to false,
+   | maven will use a sensible default value, perhaps based on some other setting, for
+   | the parameter in question.
+   |
+   | Default: true
+  <interactiveMode>true</interactiveMode>
+  -->
+
+  <!-- offline
+   | Determines whether maven should attempt to connect to the network when executing a build.
+   | This will have an effect on artifact downloads, artifact deployment, and others.
+   |
+   | Default: false
+  <offline>false</offline>
+  -->
+
+  <!-- pluginGroups
+   | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e.
+   | when invoking a command line like "mvn prefix:goal". Maven will automatically add the group identifiers
+   | "org.apache.maven.plugins" and "org.codehaus.mojo" if these are not already contained in the list.
+   |-->
+  <pluginGroups>
+    <!-- pluginGroup
+     | Specifies a further group identifier to use for plugin lookup.
+    <pluginGroup>com.your.plugins</pluginGroup>
+    -->
+  </pluginGroups>
+
+  <!-- TODO Since when can proxies be selected as depicted? -->
+  <!-- proxies
+   | This is a list of proxies which can be used on this machine to connect to the network.
+   | Unless otherwise specified (by system property or command-line switch), the first proxy
+   | specification in this list marked as active will be used.
+   |-->
+  <proxies>
+    <!-- proxy
+     | Specification for one proxy, to be used in connecting to the network.
+     |
+    <proxy>
+      <id>optional</id>
+      <active>true</active>
+      <protocol>http</protocol>
+      <username>proxyuser</username>
+      <password>proxypass</password>
+      <host>proxy.host.net</host>
+      <port>80</port>
+      <nonProxyHosts>local.net|some.host.com</nonProxyHosts>
+    </proxy>
+    -->
+  </proxies>
+
+  <!-- servers
+   | This is a list of authentication profiles, keyed by the server-id used within the system.
+   | Authentication profiles can be used whenever maven must make a connection to a remote server.
+   |-->
+  <servers>
+    <!-- server
+     | Specifies the authentication information to use when connecting to a particular server, identified by
+     | a unique name within the system (referred to by the 'id' attribute below).
+     |
+     | NOTE: You should either specify username/password OR privateKey/passphrase, since these pairings are
+     |       used together.
+     |
+    <server>
+      <id>deploymentRepo</id>
+      <username>repouser</username>
+      <password>repopwd</password>
+    </server>
+    -->
+
+    <!-- Another sample, using keys to authenticate.
+    <server>
+      <id>siteServer</id>
+      <privateKey>/path/to/private/key</privateKey>
+      <passphrase>optional; leave empty if not used.</passphrase>
+    </server>
+    -->
+  </servers>
+
+  <!-- mirrors
+   | This is a list of mirrors to be used in downloading artifacts from remote repositories.
+   |
+   | It works like this: a POM may declare a repository to use in resolving certain artifacts.
+   | However, this repository may have problems with heavy traffic at times, so people have mirrored
+   | it to several places.
+   |
+   | That repository definition will have a unique id, so we can create a mirror reference for that
+   | repository, to be used as an alternate download site. The mirror site will be the preferred
+   | server for that repository.
+   |-->
+  <mirrors>
+    <!-- mirror
+     | Specifies a repository mirror site to use instead of a given repository. The repository that
+     | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used
+     | for inheritance and direct lookup purposes, and must be unique across the set of mirrors.
+     |
+    <mirror>
+      <id>mirrorId</id>
+      <mirrorOf>repositoryId</mirrorOf>
+      <name>Human Readable Name for this Mirror.</name>
+      <url>http://my.repository.com/repo/path</url>
+    </mirror>
+     -->
+<!--     <mirror>
+      <id>maven-default-http-blocker</id>
+      <mirrorOf>external:http:*</mirrorOf>
+      <name>Pseudo repository to mirror external repositories initially using HTTP.</name>
+      <url>http://0.0.0.0/</url>
+      <blocked>true</blocked>
+    </mirror> -->
+    <mirror>
+      <id>aliyunmaven</id>
+      <mirrorOf>*</mirrorOf>
+      <name>aliyunmaven</name>
+      <url>https://maven.aliyun.com/repository/public</url>
+    </mirror>
+  </mirrors>
+
+  <!-- profiles
+   | This is a list of profiles which can be activated in a variety of ways, and which can modify
+   | the build process. Profiles provided in the settings.xml are intended to provide local machine-
+   | specific paths and repository locations which allow the build to work in the local environment.
+   |
+   | For example, if you have an integration testing plugin - like cactus - that needs to know where
+   | your Tomcat instance is installed, you can provide a variable here such that the variable is
+   | dereferenced during the build process to configure the cactus plugin.
+   |
+   | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles
+   | section of this document (settings.xml) - will be discussed later. Another way essentially
+   | relies on the detection of a property, either matching a particular value for the property,
+   | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a
+   | value of '1.4' might activate a profile when the build is executed on a JDK version of '1.4.2_07'.
+   | Finally, the list of active profiles can be specified directly from the command line.
+   |
+   | NOTE: For profiles defined in the settings.xml, you are restricted to specifying only artifact
+   |       repositories, plugin repositories, and free-form properties to be used as configuration
+   |       variables for plugins in the POM.
+   |
+   |-->
+  <profiles>
+    <!-- profile
+     | Specifies a set of introductions to the build process, to be activated using one or more of the
+     | mechanisms described above. For inheritance purposes, and to activate profiles via <activatedProfiles/>
+     | or the command line, profiles have to have an ID that is unique.
+     |
+     | An encouraged best practice for profile identification is to use a consistent naming convention
+     | for profiles, such as 'env-dev', 'env-test', 'env-production', 'user-jdcasey', 'user-brett', etc.
+     | This will make it more intuitive to understand what the set of introduced profiles is attempting
+     | to accomplish, particularly when you only have a list of profile id's for debug.
+     |
+     | This profile example uses the JDK version to trigger activation, and provides a JDK-specific repo.
+    <profile>
+      <id>jdk-1.4</id>
+
+      <activation>
+        <jdk>1.4</jdk>
+      </activation>
+
+      <repositories>
+        <repository>
+          <id>jdk14</id>
+          <name>Repository for JDK 1.4 builds</name>
+          <url>http://www.myhost.com/maven/jdk14</url>
+          <layout>default</layout>
+          <snapshotPolicy>always</snapshotPolicy>
+        </repository>
+      </repositories>
+    </profile>
+    -->
+
+    <!--
+     | Here is another profile, activated by the property 'target-env' with a value of 'dev', which
+     | provides a specific path to the Tomcat instance. To use this, your plugin configuration might
+     | hypothetically look like:
+     |
+     | ...
+     | <plugin>
+     |   <groupId>org.myco.myplugins</groupId>
+     |   <artifactId>myplugin</artifactId>
+     |
+     |   <configuration>
+     |     <tomcatLocation>${tomcatPath}</tomcatLocation>
+     |   </configuration>
+     | </plugin>
+     | ...
+     |
+     | NOTE: If you just wanted to inject this configuration whenever someone set 'target-env' to
+     |       anything, you could just leave off the <value/> inside the activation-property.
+     |
+    <profile>
+      <id>env-dev</id>
+
+      <activation>
+        <property>
+          <name>target-env</name>
+          <value>dev</value>
+        </property>
+      </activation>
+
+      <properties>
+        <tomcatPath>/path/to/tomcat/instance</tomcatPath>
+      </properties>
+    </profile>
+    -->
+  </profiles>
+
+  <!-- activeProfiles
+   | List of profiles that are active for all builds.
+   |
+  <activeProfiles>
+    <activeProfile>alwaysActiveProfile</activeProfile>
+    <activeProfile>anotherAlwaysActiveProfile</activeProfile>
+  </activeProfiles>
+  -->
+</settings>
\ No newline at end of file
diff --git a/patch/vsag.patch b/patch/vsag.patch
new file mode 100644
index 0000000..08bdac9
--- /dev/null
+++ b/patch/vsag.patch
@@ -0,0 +1,1290 @@
+diff --git a/CMakeLists.txt b/CMakeLists.txt
+index 87d091b..2eee8a5 100644
+--- a/CMakeLists.txt
++++ b/CMakeLists.txt
+@@ -39,8 +39,8 @@ option (ENABLE_FRAME_POINTER "Whether to build with -fno-omit-frame-pointer" ON)
+ option (ENABLE_THIN_LTO "Whether to build with thin lto -flto=thin" OFF)
+ option (ENABLE_CCACHE "Whether to open ccache" OFF)
+ option (ENABLE_COVERAGE "Enable gcov (debug, Linux builds only)" OFF)
+-option (ENABLE_INTEL_MKL "Enable intel-mkl (x86 platform only)" ON)
+-option (ENABLE_CXX11_ABI "Use CXX11 ABI" ON)
++option (ENABLE_INTEL_MKL "Enable intel-mkl (x86 platform only)" OFF)
++option (ENABLE_CXX11_ABI "Use CXX11 ABI" OFF)
+ option (ENABLE_LIBCXX "Use libc++ instead of libstdc++" ON)
+ option (ENABLE_TOOLS "Whether compile vsag tools" ON)
+ option (DISABLE_SSE_FORCE "Force disable sse and higher instructions" OFF)
+@@ -48,6 +48,9 @@ option (DISABLE_AVX_FORCE "Force disable avx and higher instructions" OFF)
+ option (DISABLE_AVX2_FORCE "Force disable avx2 and higher instructions" OFF)
+ option (DISABLE_AVX512_FORCE "Force disable avx512 instructions" OFF)
+ 
++set (ROARING_DISABLE_AVX512 ON)
++set (ENABLE_PYBINDS ON)
++
+ if (ENABLE_CXX11_ABI)
+   add_definitions (-D_GLIBCXX_USE_CXX11_ABI=1)
+ else ()
+@@ -282,3 +285,32 @@ add_custom_target (version
+                    -P ${CMAKE_CURRENT_SOURCE_DIR}/cmake/GenerateVersionHeader.cmake
+   )
+ add_dependencies (vsag version)
++
++# set path
++set(VSAG_BASE_DIR ${CMAKE_BINARY_DIR})
++set(VSAG_LIB_DIR ${VSAG_BASE_DIR}/_deps)
++
++# Create shared library
++add_library(ob_vsag SHARED ob_vsag_lib.cpp)
++target_compile_options(ob_vsag PRIVATE -std=c++17)
++target_include_directories(ob_vsag PRIVATE
++                           ${VSAG_BASE_DIR}/include
++                          ${VSAG_LIB_DIR}/nlohmann_json-src/include
++                           ${VSAG_LIB_DIR}/roaringbitmap-src/include
++                           ${VSAG_LIB_DIR}/fmt-src/include)
++target_compile_definitions(ob_vsag PRIVATE _GLIBCXX_USE_CXX11_ABI=0)
++target_link_libraries(ob_vsag PUBLIC vsag_static -static-libstdc++ -static-libgcc)
++add_dependencies(ob_vsag vsag_static)
++
++# Create static library
++add_library(ob_vsag_static STATIC ob_vsag_lib.cpp)
++target_compile_options(ob_vsag_static PRIVATE -std=c++17)
++target_compile_definitions(ob_vsag_static PUBLIC _GLIBCXX_USE_CXX11_ABI=0)
++target_include_directories(ob_vsag_static PUBLIC
++                           ${VSAG_BASE_DIR}/include
++                          ${VSAG_LIB_DIR}/nlohmann_json-src/include
++                           ${VSAG_LIB_DIR}/roaringbitmap-src/include
++                           ${VSAG_LIB_DIR}/fmt-src/include)
++add_dependencies(ob_vsag vsag_static)
++
++add_subdirectory (example)
+\ No newline at end of file
+diff --git a/default_logger.h b/default_logger.h
+new file mode 100644
+index 0000000..6ec885d
+--- /dev/null
++++ b/default_logger.h
+@@ -0,0 +1,119 @@
++#ifndef DEFAULT_LOGGER_H
++#define DEFAULT_LOGGER_H
++
++#include "vsag/logger.h"
++#include "vsag/options.h"
++#include "fmt/format.h"
++
++namespace vsag {
++namespace logger {
++
++enum class level {
++    trace = Logger::Level::kTRACE,
++    debug = Logger::Level::kDEBUG,
++    info = Logger::Level::kINFO,
++    warn = Logger::Level::kWARN,
++    err = Logger::Level::kERR,
++    critical = Logger::Level::kCRITICAL,
++    off = Logger::Level::kOFF
++};
++
++class ObDefaultLogger : public Logger {
++public:
++    void
++    SetLevel(Logger::Level log_level) override;
++
++    void
++    Trace(const std::string& msg) override;
++
++    void
++    Debug(const std::string& msg) override;
++
++    void
++    Info(const std::string& msg) override;
++
++    void
++    Warn(const std::string& msg) override;
++
++    void
++    Error(const std::string& msg) override;
++
++    void
++    Critical(const std::string& msg) override;
++};
++
++
++inline void
++set_level(level log_level) {
++    Options::Instance().logger()->SetLevel((Logger::Level)log_level);
++}
++
++inline void
++trace(const std::string& msg) {
++    Options::Instance().logger()->Trace(msg);
++}
++
++inline void
++debug(const std::string& msg) {
++    Options::Instance().logger()->Debug(msg);
++}
++
++inline void
++info(const std::string& msg) {
++    Options::Instance().logger()->Info(msg);
++}
++
++inline void
++warn(const std::string& msg) {
++    Options::Instance().logger()->Warn(msg);
++}
++
++inline void
++error(const std::string& msg) {
++    Options::Instance().logger()->Error(msg);
++}
++
++inline void
++critical(const std::string& msg) {
++    Options::Instance().logger()->Critical(msg);
++}
++
++template <typename... Args>
++inline void
++trace(fmt::format_string<Args...> fmt, Args&&... args) {
++    trace(fmt::format(fmt, std::forward<Args>(args)...));
++}
++
++template <typename... Args>
++inline void
++debug(fmt::format_string<Args...> fmt, Args&&... args) {
++    debug(fmt::format(fmt, std::forward<Args>(args)...));
++}
++
++template <typename... Args>
++inline void
++info(fmt::format_string<Args...> fmt, Args&&... args) {
++    info(fmt::format(fmt, std::forward<Args>(args)...));
++}
++
++template <typename... Args>
++inline void
++warn(fmt::format_string<Args...> fmt, Args&&... args) {
++    warn(fmt::format(fmt, std::forward<Args>(args)...));
++}
++
++template <typename... Args>
++inline void
++error(fmt::format_string<Args...> fmt, Args&&... args) {
++    error(fmt::format(fmt, std::forward<Args>(args)...));
++}
++
++template <typename... Args>
++inline void
++critical(fmt::format_string<Args...> fmt, Args&&... args) {
++    critical(fmt::format(fmt, std::forward<Args>(args)...));
++}
++
++}  // namespace logger
++}  // namespace vsag
++#endif //DEFAULT_LOGGER_H
+diff --git a/example/CMakeLists.txt b/example/CMakeLists.txt
+new file mode 100644
+index 0000000..0629034
+--- /dev/null
++++ b/example/CMakeLists.txt
+@@ -0,0 +1,4 @@
++add_executable(hnsw_example hnsw_example.cpp default_allocator.cpp)
++target_compile_options(hnsw_example PRIVATE -std=c++17)
++target_link_libraries(hnsw_example PRIVATE ob_vsag_static vsag dl roaring fmt)
++target_include_directories(hnsw_example BEFORE PRIVATE ${VSAG_LIB_DIR}/_deps/roaringbitmap-src/include/)
+diff --git a/example/default_allocator.cpp b/example/default_allocator.cpp
+new file mode 100644
+index 0000000..44bf49f
+--- /dev/null
++++ b/example/default_allocator.cpp
+@@ -0,0 +1,60 @@
++#include "default_allocator.h"
++
++#include "vsag/options.h"
++#include "../default_logger.h"
++#include <stdio.h>
++
++#include <stdlib.h>
++#include <unistd.h>
++void *_malloc(size_t size, const char *filename, int line){
++    void *ptr = malloc(size);
++    
++    char buffer[128] = {0};
++    sprintf(buffer, "./%p.memory", ptr);
++
++    FILE *fp = fopen(buffer, "w");
++    fprintf(fp, "[+]addr: %p, filename: %s, line: %d\n", ptr, filename, line);
++
++    fflush(fp);
++    fclose(fp);
++
++    return ptr;
++}
++
++void _free(void *ptr, const char *filename, int line){
++    char buffer[1280] = {0};
++    sprintf(buffer, "./memory/%p.memory", ptr);
++
++    if (unlink(buffer) < 0){
++        printf("double free: %p\n", ptr);
++        return;
++    }
++
++    return free(ptr);
++}
++//#define malloc(size)    _malloc(size, __FILE__, __LINE__)
++//#define free(ptr)       _free(ptr, __FILE__, __LINE__)
++void*
++DefaultAllocator::Allocate(size_t size) {
++    void* ptr = malloc(size);
++    //vsag::logger::debug("allocate memoery,addr:{}, size:{}",ptr, size);
++    return malloc(size);
++}
++
++void
++DefaultAllocator::Deallocate(void* p) {
++    //vsag::logger::debug("free memoery, alloctor:{}, free_addr:{}", (void*)this, p);
++    free(p);
++}
++
++void*
++DefaultAllocator::Reallocate(void* p, size_t size) {
++    //vsag::logger::debug("re-allocate memoery,addr:{}, size:{}",p,size);
++    return realloc(p, size);
++}
++
++std::string
++DefaultAllocator::Name() {
++    return "DefaultAllocator";
++}
++
+diff --git a/example/default_allocator.h b/example/default_allocator.h
+new file mode 100644
+index 0000000..9d26252
+--- /dev/null
++++ b/example/default_allocator.h
+@@ -0,0 +1,26 @@
++#ifndef DEFAULT_ALLOCATOR_H
++#define DEFAULT_ALLOCATOR_H
++#include "vsag/allocator.h"
++
++class DefaultAllocator : public vsag::Allocator {
++public:
++    DefaultAllocator() = default;
++    virtual ~DefaultAllocator() = default;
++
++    DefaultAllocator(const DefaultAllocator&) = delete;
++    DefaultAllocator(DefaultAllocator&&) = delete;
++
++public:
++    std::string
++    Name() override;
++
++    void*
++    Allocate(size_t size) override;
++
++    void
++    Deallocate(void* p) override;
++
++    void*
++    Reallocate(void* p, size_t size) override;
++};
++#endif // DEFAULT_ALLOCATOR_H
+diff --git a/example/hnsw_example.cpp b/example/hnsw_example.cpp
+new file mode 100644
+index 0000000..a8e2f9b
+--- /dev/null
++++ b/example/hnsw_example.cpp
+@@ -0,0 +1,255 @@
++#include "../ob_vsag_lib.h"
++#include "default_allocator.h"
++#include <random>
++#include <dlfcn.h>
++#include "../ob_vsag_lib_c.h"
++#include <iostream>
++#include "../default_logger.h"
++#include "roaring/roaring64.h"
++#include <stdio.h>
++#include <stdlib.h>
++
++
++int64_t example() {
++    std::cout<<"test hnsw_example: "<<std::endl;
++    bool is_init = obvectorlib::is_init();
++    //set_log_level(1);
++    obvectorlib::VectorIndexPtr index_handler = NULL;
++    int dim = 128;
++    int max_degree = 16;
++    int ef_search = 200;
++    int ef_construction = 100;
++    DefaultAllocator default_allocator;
++    const char* const METRIC_L2 = "l2";
++    const char* const METRIC_IP = "ip";
++
++    const char* const DATATYPE_FLOAT32 = "float32";
++    void * test_ptr = default_allocator.Allocate(10);
++    int ret_create_index = obvectorlib::create_index(index_handler,
++                                                     obvectorlib::HNSW_TYPE,
++                                                     DATATYPE_FLOAT32,
++                                                     METRIC_IP,
++                                                     dim,
++                                                     max_degree,
++                                                     ef_construction,
++                                                     ef_search,
++                                                     &default_allocator);
++   
++    if (ret_create_index!=0) return 333;
++    int num_vectors = 10;
++    auto ids = new int64_t[num_vectors];
++    auto vectors = new float[dim * num_vectors];
++    std::mt19937 rng;
++    rng.seed(47);
++    std::uniform_real_distribution<> distrib_real;
++    for (int64_t i = 0; i < num_vectors; ++i) {
++        ids[i] = i*2;
++    }
++    for (int64_t i = 0; i < dim * num_vectors; ++i) {
++        vectors[i] = distrib_real(rng);
++    }
++    int ret_build_index = obvectorlib::build_index(index_handler, vectors, ids, dim, num_vectors);
++
++    int64_t num_size = 0;
++    int ret_get_element = obvectorlib::get_index_number(index_handler, num_size);
++    std::cout<<"after add index, size is "<<num_size<<" " <<ret_get_element<<std::endl;
++
++    int inc_num = 1000;
++    auto inc = new float[dim * inc_num];
++    for (int64_t i = 0; i < dim * inc_num; ++i) {
++        inc[i] = distrib_real(rng);
++    }
++    auto ids2 = new int64_t[inc_num];
++    for (int64_t i = 0; i < inc_num; ++i) {
++        ids2[i] = i*2+1;
++    }
++    
++    int ret_add_index = obvectorlib::add_index(index_handler, inc, ids2, dim,inc_num);
++    ret_get_element = obvectorlib::get_index_number(index_handler, num_size);
++    std::cout<<"after add index, size is "<<num_size<<" " <<ret_add_index<<std::endl;
++    
++    const float* result_dist;
++    const int64_t* result_ids;
++    int64_t result_size = 0;
++
++    roaring::api::roaring64_bitmap_t* r1 = roaring::api::roaring64_bitmap_create();
++
++    roaring::api::roaring64_bitmap_add(r1, 18);
++    roaring::api::roaring64_bitmap_add(r1, 1169);
++    roaring::api::roaring64_bitmap_add(r1, 1285);
++
++    int ret_knn_search = obvectorlib::knn_search(index_handler, vectors+dim*(num_vectors-1), dim, 10,
++                                                 result_dist,result_ids,result_size, 
++                                                 100, r1);
++    const std::string dir = "./";
++    int ret_serialize_single = obvectorlib::serialize(index_handler,dir);
++    int ret_deserilize_single_bin = 
++                    obvectorlib::deserialize_bin(index_handler,dir);
++ ret_knn_search = obvectorlib::knn_search(index_handler, vectors+dim*(num_vectors-1), dim, 10,
++                                                 result_dist,result_ids,result_size, 
++                                                 100, r1);
++     obvectorlib::delete_index(index_handler);
++    free(test_ptr);
++    return 0;
++}
++
++void
++vsag::logger::ObDefaultLogger::SetLevel(Logger::Level log_level) {
++    //
++}
++
++void
++vsag::logger::ObDefaultLogger::Trace(const std::string& msg) {
++    //
++}
++
++void
++vsag::logger::ObDefaultLogger::Debug(const std::string& msg) {
++    //
++}
++
++void
++vsag::logger::ObDefaultLogger::Info(const std::string& msg) {
++    //
++}
++
++void
++vsag::logger::ObDefaultLogger::Warn(const std::string& msg) {
++    //
++}
++
++void
++vsag::logger::ObDefaultLogger::Error(const std::string& msg) {
++    //
++}
++
++void
++vsag::logger::ObDefaultLogger::Critical(const std::string& msg) {
++    //
++}
++
++int example_so() {
++    std::cout<<"test hnsw_example with dlopen: "<<std::endl;
++    // Path to the dynamic library
++    const char* lib_path = "./libob_vsag.so";  // Linux
++    // const char* lib_path = "libexample.dylib";  // macOS
++
++    // Open the dynamic library
++    void* handle = dlopen(lib_path, RTLD_LAZY);
++    if (!handle) {
++        fprintf(stderr, "%s\n", dlerror());
++        return EXIT_FAILURE;
++    }
++    
++
++    obvectorlib::set_logger_ptr set_logger_c;
++    LOAD_FUNCTION(handle, obvectorlib::set_logger_ptr, set_logger_c);
++    //void* raw_memory = (void*)malloc(sizeof( vsag::logger::ObDefaultLogger));
++    //vsag::logger::ObDefaultLogger* ob_logger = new (raw_memory)vsag::logger::ObDefaultLogger();
++    //vsag::logger::ObDefaultLogger* ob_logger = new vsag::logger::ObDefaultLogger();
++    //set_logger_c(ob_logger);
++
++    //init
++    //obvectorlib::is_init_ptr is_init_c;
++    //LOAD_FUNCTION(handle, obvectorlib::is_init_ptr, is_init_c);
++    //bool is_vsag_init_ = is_init_c();
++    //std::cout << "is_vsag_init_: " << is_vsag_init_ << std::endl;
++
++    //create index
++    obvectorlib::create_index_ptr create_index_c;
++    LOAD_FUNCTION(handle, obvectorlib::create_index_ptr, create_index_c);
++    obvectorlib::VectorIndexPtr index_handler = NULL;
++    int dim = 128;
++    int max_degree = 16;
++    int ef_search = 200;
++    int ef_construction = 100;
++    DefaultAllocator default_allocator;
++    const char* const METRIC_L2 = "l2";
++    const char* const DATATYPE_FLOAT32 = "float32";
++    int ret_create_index = create_index_c(index_handler,
++                                                     obvectorlib::HNSW_TYPE,
++                                                     DATATYPE_FLOAT32,
++                                                     METRIC_L2,
++                                                     dim,
++                                                     max_degree,
++                                                     ef_construction,
++                                                     ef_search,
++                                                     &default_allocator);
++
++    //build index
++    obvectorlib::build_index_ptr build_index_c;
++    LOAD_FUNCTION(handle, obvectorlib::build_index_ptr, build_index_c);
++    obvectorlib::get_index_number_ptr get_index_number_c;
++    LOAD_FUNCTION(handle, obvectorlib::get_index_number_ptr, get_index_number_c);
++    int num_vectors = 10000;
++    auto ids = new int64_t[num_vectors];
++    auto vectors = new float[dim * num_vectors];
++    std::mt19937 rng;
++    rng.seed(47);
++    std::uniform_real_distribution<> distrib_real;
++    for (int64_t i = 0; i < num_vectors; ++i) {
++        ids[i] = i;
++    }
++    for (int64_t i = 0; i < dim * num_vectors; ++i) {
++        vectors[i] = distrib_real(rng);
++    }
++    int ret_build_index = build_index_c(index_handler, vectors, ids, dim, num_vectors);
++    
++    int64_t num_size = 0;
++    int ret_get_element = get_index_number_c(index_handler, num_size);
++
++    //add index
++    obvectorlib::add_index_ptr add_index_c;
++    LOAD_FUNCTION(handle, obvectorlib::add_index_ptr, add_index_c);
++    int inc_num = 10000;
++    auto inc = new float[dim * inc_num];
++    for (int64_t i = 0; i < dim * inc_num; ++i) {
++        inc[i] = distrib_real(rng);
++    }
++    auto ids2 = new int64_t[inc_num];
++    for (int64_t i = 0; i < inc_num; ++i) {
++        ids2[i] = num_size+i;
++    }
++    
++    int ret_add_index = add_index_c(index_handler, inc, ids2, dim,inc_num);
++    ret_get_element = get_index_number_c(index_handler, num_size);
++    
++    //knn_search
++    obvectorlib::knn_search_ptr knn_search_c;
++    LOAD_FUNCTION(handle, obvectorlib::knn_search_ptr, knn_search_c);
++    const float* result_dist;
++    const int64_t* result_ids;
++    int64_t result_size = 0;
++
++    roaring::api::roaring64_bitmap_t* r1 = roaring::api::roaring64_bitmap_create();
++
++    roaring::api::roaring64_bitmap_add(r1, 9999);
++    roaring::api::roaring64_bitmap_add(r1, 1169);
++    roaring::api::roaring64_bitmap_add(r1, 1285);
++
++    int ret_knn_search = knn_search_c(index_handler, vectors+dim*(num_vectors-1), dim, 10,
++                                                 result_dist,result_ids,result_size, 
++                                                 100, r1);
++
++    //serialize/deserialize
++    obvectorlib::serialize_ptr serialize_c;
++    LOAD_FUNCTION(handle, obvectorlib::serialize_ptr, serialize_c);
++    obvectorlib::deserialize_bin_ptr deserialize_bin_c;
++    LOAD_FUNCTION(handle, obvectorlib::deserialize_bin_ptr, deserialize_bin_c);
++    const std::string dir = "./";
++    int ret_serialize_single = serialize_c(index_handler,dir);
++    int ret_deserilize_single_bin = deserialize_bin_c(index_handler,dir);
++
++
++    // Clean up
++    dlclose(handle);
++    
++    return 0;
++}
++
++int
++main() {
++    example();
++    example_so();
++    return 0;
++}
+diff --git a/ob_vsag_lib.cpp b/ob_vsag_lib.cpp
+new file mode 100644
+index 0000000..1e0eaa6
+--- /dev/null
++++ b/ob_vsag_lib.cpp
+@@ -0,0 +1,568 @@
++
++#include "ob_vsag_lib.h"
++#include "ob_vsag_lib_c.h"
++#include "nlohmann/json.hpp"
++#include "roaring/roaring64.h"
++#include <vsag/vsag.h>
++#include "vsag/errors.h"
++#include "vsag/dataset.h"
++#include "vsag/bitset.h"
++#include "vsag/allocator.h"
++#include "vsag/factory.h"
++#include "vsag/constants.h"
++
++#include "default_logger.h"
++#include "vsag/logger.h"
++
++#include <fstream>
++#include <chrono>
++
++namespace obvectorlib {
++
++struct SlowTaskTimer {
++    SlowTaskTimer(const std::string& name, int64_t log_threshold_ms = 0);
++    ~SlowTaskTimer();
++
++    std::string name;
++    int64_t threshold;
++    std::chrono::steady_clock::time_point start;
++};
++
++SlowTaskTimer::SlowTaskTimer(const std::string& n, int64_t log_threshold_ms)
++    : name(n), threshold(log_threshold_ms) {
++    start = std::chrono::steady_clock::now();
++}
++
++SlowTaskTimer::~SlowTaskTimer() {
++    auto finish = std::chrono::steady_clock::now();
++    std::chrono::duration<double, std::milli> duration = finish - start;
++    if (duration.count() > threshold) {
++        if (duration.count() >= 1000) {
++            vsag::logger::debug("  {0} cost {1:.3f}s", name, duration.count() / 1000);
++        } else {
++            vsag::logger::debug("  {0} cost {1:.3f}ms", name, duration.count());
++        }
++    }
++}
++
++class HnswIndexHandler
++{
++public:
++  HnswIndexHandler() = delete;
++
++  HnswIndexHandler(bool is_create, bool is_build, bool use_static,
++                   int max_degree, int ef_construction, int ef_search, int dim,
++                   std::shared_ptr<vsag::Index> index, vsag::Allocator* allocator):
++      is_created_(is_create),
++      is_build_(is_build),
++      use_static_(use_static),
++      max_degree_(max_degree),
++      ef_construction_(ef_construction),
++      ef_search_(ef_search),
++      dim_(dim),
++      index_(index),
++      allocator_(allocator)
++  {}
++
++  ~HnswIndexHandler() {
++    index_ = nullptr;
++    vsag::logger::debug("   after deconstruction, hnsw index addr {} : use count {}", (void*)allocator_, index_.use_count());
++  }
++  void set_build(bool is_build) { is_build_ = is_build;}
++  bool is_build(bool is_build) { return is_build_;}
++  int build_index(const vsag::DatasetPtr& base);
++  int get_index_number();
++  int add_index(const vsag::DatasetPtr& incremental);
++  int knn_search(const vsag::DatasetPtr& query, int64_t topk,
++                const std::string& parameters,
++                const float*& dist, const int64_t*& ids, int64_t &result_size,
++                const std::function<bool(int64_t)>& filter);
++  std::shared_ptr<vsag::Index>& get_index() {return index_;}
++  void set_index(std::shared_ptr<vsag::Index> hnsw) {index_ = hnsw;}
++  vsag::Allocator* get_allocator() {return allocator_;}
++  inline bool get_use_static() {return use_static_;}
++  inline int get_max_degree() {return max_degree_;}
++  inline int get_ef_construction() {return ef_construction_;}
++  inline int get_ef_search() {return ef_search_;}
++  inline int get_dim() {return dim_;}
++  
++private:
++  bool is_created_;
++  bool is_build_;
++  bool use_static_;
++  int max_degree_;
++  int ef_construction_;
++  int ef_search_;
++  int dim_;
++  std::shared_ptr<vsag::Index> index_;
++  vsag::Allocator* allocator_;
++};
++
++int HnswIndexHandler::build_index(const vsag::DatasetPtr& base) 
++{
++    vsag::ErrorType error = vsag::ErrorType::UNKNOWN_ERROR;
++    if (const auto num = index_->Build(base); num.has_value()) {
++        return 0;
++    } else {
++        error = num.error().type;
++    }
++    return static_cast<int>(error);
++}
++
++int HnswIndexHandler::get_index_number() 
++{
++    return index_->GetNumElements();
++}
++
++int HnswIndexHandler::add_index(const vsag::DatasetPtr& incremental) 
++{
++    vsag::ErrorType error = vsag::ErrorType::UNKNOWN_ERROR;
++    if (const auto num = index_->Add(incremental); num.has_value()) {
++        vsag::logger::debug(" after add index, index count {}", get_index_number());
++        return 0;
++    } else {
++        error = num.error().type;
++    }
++    return static_cast<int>(error);
++}
++
++int HnswIndexHandler::knn_search(const vsag::DatasetPtr& query, int64_t topk,
++               const std::string& parameters,
++               const float*& dist, const int64_t*& ids, int64_t &result_size,
++               const std::function<bool(int64_t)>& filter) {
++    vsag::logger::debug("  search_parameters:{}", parameters);
++    vsag::logger::debug("  topk:{}", topk);
++    vsag::ErrorType error = vsag::ErrorType::UNKNOWN_ERROR;
++
++    auto result = index_->KnnSearch(query, topk, parameters, filter);
++    if (result.has_value()) {
++        //result
++        result.value()->Owner(false);
++        ids = result.value()->GetIds();
++        dist = result.value()->GetDistances();
++        result_size = result.value()->GetDim();
++        // print the results
++        for (int64_t i = 0; i < result_size; ++i) {
++            vsag::logger::debug("  knn search id : {}, distance : {}",ids[i],dist[i]);
++        }
++        return 0; 
++    } else {
++        error = result.error().type;
++    }
++
++    return static_cast<int>(error);
++}
++
++bool is_init_ = vsag::init();
++
++void
++set_log_level(int64_t level_num) {
++    vsag::Logger::Level log_level = static_cast<vsag::Logger::Level>(level_num);
++    vsag::Options::Instance().logger()->SetLevel(log_level);
++}
++
++bool is_init() {
++    vsag::logger::debug("TRACE LOG[Init VsagLib]:");
++    if (is_init_) {
++        vsag::logger::debug("   Init VsagLib success");
++    } else {
++        vsag::logger::debug("   Init VsagLib fail");
++    }
++    return is_init_; 
++}
++
++
++void set_logger(void *logger_ptr) {
++    vsag::Options::Instance().set_logger(static_cast<vsag::Logger*>(logger_ptr));
++    vsag::Logger::Level log_level = static_cast<vsag::Logger::Level>(1);//default is debug level
++    vsag::Options::Instance().logger()->SetLevel(log_level);
++}
++
++void set_block_size_limit(uint64_t size) {
++    vsag::Options::Instance().set_block_size_limit(size);
++}
++
++bool is_supported_index(IndexType index_type) {
++    return INVALID_INDEX_TYPE < index_type && index_type < MAX_INDEX_TYPE;
++}
++
++int create_index(VectorIndexPtr& index_handler, IndexType index_type,
++                 const char* dtype,
++                 const char* metric, int dim,
++                 int max_degree, int ef_construction, int ef_search, void* allocator)
++{   
++    vsag::logger::debug("TRACE LOG[create_index]:");
++    vsag::ErrorType error = vsag::ErrorType::UNKNOWN_ERROR;
++    int ret = 0;
++    if (dtype == nullptr || metric == nullptr) {
++        vsag::logger::debug("   null pointer addr, dtype:{}, metric:{}", (void*)dtype, (void*)metric);
++        return static_cast<int>(error);
++    }
++    SlowTaskTimer t("create_index");
++    vsag::Allocator* vsag_allocator = NULL;
++    bool is_support = is_supported_index(index_type);
++    vsag::logger::debug("   index type : {}, is_supported : {}", static_cast<int>(index_type), is_support);
++    if (allocator == NULL) {
++        vsag_allocator = NULL;
++        vsag::logger::debug("   allocator is null ,use default_allocator");
++    } else {
++        vsag_allocator =  static_cast<vsag::Allocator*>(allocator);
++        vsag::logger::debug("   allocator_addr:{}",allocator);
++    }
++
++    if (is_support) {
++        // create index
++        std::shared_ptr<vsag::Index> hnsw;
++        bool use_static = false;
++        nlohmann::json hnsw_parameters{{"max_degree", max_degree},
++                                {"ef_construction", ef_construction},
++                                {"ef_search", ef_search},
++                                {"use_static", use_static}};
++        nlohmann::json index_parameters{{"dtype", dtype}, {"metric_type", metric}, {"dim", dim}, {"hnsw", hnsw_parameters}}; 
++        if (auto index = vsag::Factory::CreateIndex("hnsw", index_parameters.dump(), vsag_allocator);
++            index.has_value()) {
++            hnsw = index.value();
++            HnswIndexHandler* hnsw_index = new HnswIndexHandler(true,
++                                                                false,
++                                                                use_static,
++                                                                max_degree,
++                                                                ef_construction,
++                                                                ef_search,
++                                                                dim,
++                                                                hnsw,
++                                                                vsag_allocator);
++            index_handler = static_cast<VectorIndexPtr>(hnsw_index);
++            vsag::logger::debug("   success to create hnsw index , index parameter:{}, allocator addr:{}",index_parameters.dump(), (void*)vsag_allocator);
++            return 0;
++        } else {
++            error = index.error().type;
++            vsag::logger::debug("   fail to create hnsw index , index parameter:{}", index_parameters.dump());
++        }
++    } else {
++        error = vsag::ErrorType::UNSUPPORTED_INDEX;
++        vsag::logger::debug("   fail to create hnsw index , index type not supported:{}", static_cast<int>(index_type));
++    }
++    ret = static_cast<int>(error);
++    if (ret != 0) {
++        vsag::logger::error("   create index error happend, ret={}", static_cast<int>(error));
++    }
++    return ret;
++}
++
++int build_index(VectorIndexPtr& index_handler,float* vector_list, int64_t* ids, int dim, int size) {
++    vsag::logger::debug("TRACE LOG[build_index]:");
++    vsag::ErrorType error = vsag::ErrorType::UNKNOWN_ERROR;
++    int ret =  0;
++    if (index_handler == nullptr || vector_list == nullptr || ids == nullptr) {
++        vsag::logger::debug("   null pointer addr, index_handler:{}, ids:{}, ids:{}",
++                                                   (void*)index_handler, (void*)vector_list, (void*)ids);
++        return static_cast<int>(error);
++    }
++    SlowTaskTimer t("build_index");
++    HnswIndexHandler* hnsw = static_cast<HnswIndexHandler*>(index_handler);
++    auto dataset = vsag::Dataset::Make();
++    dataset->Dim(dim)
++    ->NumElements(size)
++    ->Ids(ids)
++    ->Float32Vectors(vector_list)
++    ->Owner(false);
++    ret = hnsw->build_index(dataset);
++    if (ret != 0) {
++        vsag::logger::error("   build index error happend, ret={}", ret);
++    }
++    return ret;
++}
++
++
++int add_index(VectorIndexPtr& index_handler,float* vector, int64_t* ids, int dim, int size) {
++    vsag::logger::debug("TRACE LOG[add_index]:");
++    vsag::ErrorType error = vsag::ErrorType::UNKNOWN_ERROR;
++    int ret = 0;
++    if (index_handler == nullptr || vector == nullptr || ids == nullptr) {
++        vsag::logger::debug("   null pointer addr, index_handler:{}, ids:{}, ids:{}",
++                                                   (void*)index_handler, (void*)vector, (void*)ids);
++        return static_cast<int>(error);
++    }
++    HnswIndexHandler* hnsw = static_cast<HnswIndexHandler*>(index_handler);
++    SlowTaskTimer t("add_index");
++    // add index
++    auto incremental = vsag::Dataset::Make();
++        incremental->Dim(dim)
++            ->NumElements(size)
++            ->Ids(ids)
++            ->Float32Vectors(vector)
++            ->Owner(false);
++    ret = hnsw->add_index(incremental);
++    if (ret != 0) {
++        vsag::logger::error("   add index error happend, ret={}", ret);
++    }
++    return ret;
++}
++
++int get_index_number(VectorIndexPtr& index_handler, int64_t &size) {
++    vsag::ErrorType error = vsag::ErrorType::UNKNOWN_ERROR;
++    if (index_handler == nullptr) {
++        vsag::logger::debug("   null pointer addr, index_handler:{}", (void*)index_handler);
++        return static_cast<int>(error);
++    }
++    HnswIndexHandler* hnsw = static_cast<HnswIndexHandler*>(index_handler);
++    size = hnsw->get_index_number();
++    return 0;
++}
++
++int knn_search(VectorIndexPtr& index_handler,float* query_vector,int dim, int64_t topk,
++               const float*& dist, const int64_t*& ids, int64_t &result_size, int ef_search,
++               void* invalid) {
++    vsag::logger::debug("TRACE LOG[knn_search]:");
++    vsag::ErrorType error = vsag::ErrorType::UNKNOWN_ERROR;
++    int ret = 0;
++    if (index_handler == nullptr || query_vector == nullptr) {
++        vsag::logger::debug("   null pointer addr, index_handler:{}, query_vector:{}",
++                                                   (void*)index_handler, (void*)query_vector);
++        return static_cast<int>(error);
++    }
++    SlowTaskTimer t("knn_search");
++    roaring::api::roaring64_bitmap_t *bitmap = static_cast<roaring::api::roaring64_bitmap_t*>(invalid);
++    auto filter = [bitmap](int64_t id) -> bool {
++        return roaring::api::roaring64_bitmap_contains(bitmap, id);
++    };
++    nlohmann::json search_parameters{{"hnsw", {{"ef_search", ef_search}}}};
++    HnswIndexHandler* hnsw = static_cast<HnswIndexHandler*>(index_handler);
++    auto query = vsag::Dataset::Make();
++    query->NumElements(1)->Dim(dim)->Float32Vectors(query_vector)->Owner(false);
++    ret = hnsw->knn_search(query, topk, search_parameters.dump(), dist, ids, result_size, filter);
++    if (ret != 0) {
++        vsag::logger::error("   knn search error happend, ret={}", ret);
++    }
++    return ret;
++}
++
++int serialize(VectorIndexPtr& index_handler, const std::string dir) {
++    vsag::logger::debug("TRACE LOG[serialize]:");
++    vsag::ErrorType error = vsag::ErrorType::UNKNOWN_ERROR;
++    int ret =  0;
++    if (index_handler == nullptr) {
++        vsag::logger::debug("   null pointer addr, index_handler:{}", (void*)index_handler);
++        return static_cast<int>(error);
++    }
++    HnswIndexHandler* hnsw = static_cast<HnswIndexHandler*>(index_handler);
++    if (auto bs = hnsw->get_index()->Serialize(); bs.has_value()) {
++        hnsw = nullptr;
++        auto keys = bs->GetKeys();
++        for (auto key : keys) {
++            vsag::Binary b = bs->Get(key);
++            std::ofstream file(dir + "hnsw.index." + key, std::ios::binary);
++            file.write((const char*)b.data.get(), b.size);
++            file.close();
++        }
++        std::ofstream metafile(dir + "hnsw.index._meta", std::ios::out);
++        for (auto key : keys) {
++            metafile << key << std::endl;
++        }
++        metafile.close();
++        return 0;
++    } else {
++        error = bs.error().type;
++    }
++    ret = static_cast<int>(error);
++    if (ret != 0) {
++        vsag::logger::error("   serialize error happend, ret={}", ret);
++    }
++    return ret;
++}
++
++int fserialize(VectorIndexPtr& index_handler, std::ostream& out_stream) {
++    vsag::logger::debug("TRACE LOG[fserialize]:");
++    vsag::ErrorType error = vsag::ErrorType::UNKNOWN_ERROR;
++    int ret = 0;
++    if (index_handler == nullptr) {
++        vsag::logger::debug("   null pointer addr, index_handler:{}", (void*)index_handler);
++        return static_cast<int>(error);
++    }
++    HnswIndexHandler* hnsw = static_cast<HnswIndexHandler*>(index_handler);
++    if (auto bs = hnsw->get_index()->Serialize(out_stream); bs.has_value()) {
++        return 0;
++    } else {
++        error = bs.error().type;
++    }
++    ret = static_cast<int>(error);
++    if (ret != 0) {
++        vsag::logger::error("   fserialize error happend, ret={}", ret);
++    }
++    return ret;
++}
++
++int fdeserialize(VectorIndexPtr& index_handler, std::istream& in_stream) {
++    vsag::logger::debug("TRACE LOG[fdeserialize]:");
++    vsag::ErrorType error = vsag::ErrorType::UNKNOWN_ERROR;
++    int ret = 0;
++    if (index_handler == nullptr) {
++        vsag::logger::debug("   null pointer addr, index_handler:{}", (void*)index_handler);
++        return static_cast<int>(error);
++    }
++    HnswIndexHandler* hnsw = static_cast<HnswIndexHandler*>(index_handler);
++    std::shared_ptr<vsag::Index> hnsw_index;
++    bool use_static = hnsw->get_use_static();
++    int max_degree = hnsw->get_max_degree();
++    int ef_construction = hnsw->get_ef_construction();
++    int ef_search = hnsw->get_ef_search();
++    int dim = hnsw->get_dim();
++    nlohmann::json hnsw_parameters{{"max_degree", max_degree},
++                                {"ef_construction", ef_construction},
++                                {"ef_search", ef_search},
++                                {"use_static", use_static}};
++    nlohmann::json index_parameters{
++        {"dtype", "float32"}, {"metric_type", "l2"}, {"dim", dim}, {"hnsw", hnsw_parameters}};
++    vsag::logger::debug("   Deserilize hnsw index , index parameter:{}, allocator addr:{}",index_parameters.dump(),(void*)hnsw->get_allocator());
++    if (auto index = vsag::Factory::CreateIndex("hnsw", index_parameters.dump(), hnsw->get_allocator());
++        index.has_value()) {
++        hnsw_index = index.value();
++    } else {
++        error = index.error().type;
++        ret = static_cast<int>(error);
++    }
++    if (ret != 0) {
++
++    } else if (auto bs = hnsw_index->Deserialize(in_stream); bs.has_value()) {
++        hnsw->set_index(hnsw_index);
++        return 0;
++    } else {
++        error = bs.error().type;
++        ret = static_cast<int>(error);
++    }
++    if (ret != 0) {
++        vsag::logger::error("   fdeserialize error happend, ret={}", ret);
++    }
++    return ret;
++}
++
++int deserialize_bin(VectorIndexPtr& index_handler,const std::string dir) {
++    vsag::logger::debug("TRACE LOG[deserialize]:");
++    vsag::ErrorType error = vsag::ErrorType::UNKNOWN_ERROR;
++    int ret = 0;
++    if (index_handler == nullptr) {
++        vsag::logger::debug("   null pointer addr, index_handler={}", (void*)index_handler);
++        return static_cast<int>(error);
++    }
++    HnswIndexHandler* hnsw = static_cast<HnswIndexHandler*>(index_handler);
++    std::ifstream metafile(dir + "hnsw.index._meta", std::ios::in);
++    std::vector<std::string> keys;
++    std::string line;
++    while (std::getline(metafile, line)) {
++        keys.push_back(line);
++    }
++    metafile.close();
++
++    vsag::BinarySet bs;
++    for (auto key : keys) {
++        std::ifstream file(dir + "hnsw.index." + key, std::ios::in);
++        file.seekg(0, std::ios::end);
++        vsag::Binary b;
++        b.size = file.tellg();
++        b.data.reset(new int8_t[b.size]);
++        file.seekg(0, std::ios::beg);
++        file.read((char*)b.data.get(), b.size);
++        bs.Set(key, b);
++    }
++    bool use_static = hnsw->get_use_static();
++    int max_degree = hnsw->get_max_degree();
++    int ef_construction = hnsw->get_ef_construction();
++    int ef_search = hnsw->get_ef_search();
++    int dim = hnsw->get_dim();
++    nlohmann::json hnsw_parameters{{"max_degree", max_degree},
++                                {"ef_construction", ef_construction},
++                                {"ef_search", ef_search},
++                                {"use_static", use_static}};
++    nlohmann::json index_parameters{
++        {"dtype", "float32"}, {"metric_type", "l2"}, {"dim", dim}, {"hnsw", hnsw_parameters}};
++    vsag::logger::debug("   Deserilize hnsw index , index parameter:{}, allocator addr:{}",index_parameters.dump(),(void*)hnsw->get_allocator());
++    std::shared_ptr<vsag::Index> hnsw_index;
++    if (auto index = vsag::Factory::CreateIndex("hnsw", index_parameters.dump(),hnsw->get_allocator());
++        index.has_value()) {
++        hnsw_index = index.value();
++    } else {
++        error = index.error().type;
++        return static_cast<int>(error);
++    }
++    hnsw_index->Deserialize(bs);
++    hnsw->set_index(hnsw_index);
++    return 0;
++}
++
++int delete_index(VectorIndexPtr& index_handler) {
++    vsag::logger::debug("TRACE LOG[delete_index]");
++    vsag::logger::debug("   delete index handler addr {} : hnsw index use count {}",(void*)static_cast<HnswIndexHandler*>(index_handler)->get_index().get(),static_cast<HnswIndexHandler*>(index_handler)->get_index().use_count());
++    if (index_handler != NULL) {
++        delete static_cast<HnswIndexHandler*>(index_handler);
++        index_handler = NULL;
++    }
++    return 0;
++}
++
++int64_t example() {
++    return 0;
++}
++
++extern bool is_init_c() {
++    return is_init();
++}
++
++extern void set_logger_c(void *logger_ptr) {
++    set_logger(logger_ptr);
++}
++
++extern void set_block_size_limit_c(uint64_t size) {
++    set_block_size_limit(size);
++}
++
++extern bool is_supported_index_c(IndexType index_type) {
++    return is_supported_index(index_type);
++}
++
++extern int create_index_c(VectorIndexPtr& index_handler, IndexType index_type,
++                 const char* dtype,
++                 const char* metric, int dim,
++                 int max_degree, int ef_construction, int ef_search, void* allocator)
++{   
++    return create_index(index_handler, index_type, dtype, metric, dim, max_degree, ef_construction, ef_search, allocator);
++}
++
++extern int build_index_c(VectorIndexPtr& index_handler,float* vector_list, int64_t* ids, int dim, int size) {
++
++    return build_index(index_handler, vector_list, ids, dim, size);
++}
++
++
++extern int add_index_c(VectorIndexPtr& index_handler,float* vector, int64_t* ids, int dim, int size) {
++    return  add_index(index_handler, vector, ids, dim, size);
++}
++
++extern int get_index_number_c(VectorIndexPtr& index_handler, int64_t &size) {
++    return get_index_number(index_handler, size);
++}
++
++extern int knn_search_c(VectorIndexPtr& index_handler,float* query_vector,int dim, int64_t topk,
++               const float*& dist, const int64_t*& ids, int64_t &result_size, int ef_search, void* invalid) {
++    return knn_search(index_handler, query_vector, dim, topk, dist, ids, result_size, ef_search, invalid);
++}
++
++extern int serialize_c(VectorIndexPtr& index_handler, const std::string dir) {
++    return serialize(index_handler, dir);
++}
++
++extern int fserialize_c(VectorIndexPtr& index_handler, std::ostream& out_stream) {
++    return fserialize(index_handler, out_stream);
++}
++
++extern int delete_index_c(VectorIndexPtr& index_handler) {
++    return delete_index(index_handler);
++}
++extern int fdeserialize_c(VectorIndexPtr& index_handler, std::istream& in_stream) {
++    return fdeserialize(index_handler, in_stream);
++}
++
++extern int deserialize_bin_c(VectorIndexPtr& index_handler,const std::string dir) {
++    return deserialize_bin(index_handler, dir);
++}
++
++} //namespace obvectorlib
+\ No newline at end of file
+diff --git a/ob_vsag_lib.h b/ob_vsag_lib.h
+new file mode 100644
+index 0000000..c2b3ea1
+--- /dev/null
++++ b/ob_vsag_lib.h
+@@ -0,0 +1,62 @@
++#ifndef OB_VSAG_LIB_H
++#define OB_VSAG_LIB_H
++#include <stdint.h>
++#include <iostream>
++#include <map>
++namespace obvectorlib {
++
++
++int64_t example();
++typedef void* VectorIndexPtr;
++extern bool is_init_;
++enum IndexType {
++  INVALID_INDEX_TYPE = -1,
++  HNSW_TYPE = 0,
++  MAX_INDEX_TYPE
++};
++/**
++ *   * Get the version based on git revision
++ *     * 
++ *       * @return the version text
++ *         */
++extern std::string
++version();
++
++/**
++ *   * Init the vsag library
++ *     * 
++ *       * @return true always
++ *         */
++extern bool is_init();
++
++/*
++ * *trace = 0
++ * *debug = 1
++ * *info = 2
++ * *warn = 3
++ * *err = 4
++ * *critical = 5
++ * *off = 6
++ * */
++extern void set_log_level(int64_t level_num);
++extern void set_logger(void *logger_ptr);
++extern void set_block_size_limit(uint64_t size);
++extern bool is_supported_index(IndexType index_type);
++extern int create_index(VectorIndexPtr& index_handler, IndexType index_type,
++                        const char* dtype,
++                        const char* metric,int dim,
++                        int max_degree, int ef_construction, int ef_search, void* allocator = NULL);
++extern int build_index(VectorIndexPtr& index_handler, float* vector_list, int64_t* ids, int dim, int size);
++extern int add_index(VectorIndexPtr& index_handler, float* vector, int64_t* ids, int dim, int size);
++extern int get_index_number(VectorIndexPtr& index_handler, int64_t &size);
++extern int knn_search(VectorIndexPtr& index_handler,float* query_vector, int dim, int64_t topk,
++                      const float*& dist, const int64_t*& ids, int64_t &result_size, int ef_search,
++                       void* invalid = NULL);
++extern int serialize(VectorIndexPtr& index_handler, const std::string dir);
++extern int deserialize_bin(VectorIndexPtr& index_handler, const std::string dir);
++extern int fserialize(VectorIndexPtr& index_handler, std::ostream& out_stream);
++extern int fdeserialize(VectorIndexPtr& index_handler, std::istream& in_stream);
++extern int delete_index(VectorIndexPtr& index_handler);
++} // namesapce obvectorlib
++#endif // OB_VSAG_LIB_H
++
+diff --git a/ob_vsag_lib_c.h b/ob_vsag_lib_c.h
+new file mode 100644
+index 0000000..af46203
+--- /dev/null
++++ b/ob_vsag_lib_c.h
+@@ -0,0 +1,87 @@
++#ifndef OB_VSAG_LIB_C_H
++#define OB_VSAG_LIB_C_H
++#include "ob_vsag_lib.h"
++#include <dlfcn.h>
++// Define a macro to load function symbols
++#define LOAD_FUNCTION(handle, typeName, funcName)                             \
++    do {                                                            \
++        dlerror();  /* Clear existing errors */                     \
++        funcName = (typeName)dlsym((handle), (#funcName));          \
++        const char* dlsym_error = dlerror();                        \
++        if (dlsym_error) {                                          \
++            fprintf(stderr, "Cannot load symbol '" #funcName "': %s\n", dlsym_error); \
++            dlclose(handle);                                        \
++            return EXIT_FAILURE;                                    \
++        }                                                           \
++    } while (0)
++
++
++
++namespace obvectorlib {
++#ifdef __cplusplus
++extern "C" {
++#endif
++typedef bool (*is_init_ptr)();
++extern bool is_init_c();
++
++typedef void (*set_log_level_ptr)(int64_t level_num);
++extern void set_log_level_c(int64_t level_num);
++
++typedef void (*set_logger_ptr)(void *logger_ptr);
++extern void set_logger_c(void *logger_ptr);
++
++typedef void (*set_block_size_limit_ptr)(uint64_t size);
++extern void set_block_size_limit_c(uint64_t size);
++
++typedef bool (*is_supported_index_ptr)(IndexType index_type);
++extern bool is_supported_index_c(IndexType index_type);
++
++typedef int (*create_index_ptr)(VectorIndexPtr& index_handler, IndexType index_type,
++                        const char* dtype,
++                        const char* metric,int dim,
++                        int max_degree, int ef_construction, int ef_search, void* allocator);
++extern int create_index_c(VectorIndexPtr& index_handler, IndexType index_type,
++                        const char* dtype,
++                        const char* metric,int dim,
++                        int max_degree, int ef_construction, int ef_search, void* allocator = NULL);
++
++typedef int (*build_index_ptr)(VectorIndexPtr& index_handler, float* vector_list, int64_t* ids, int dim, int size);              
++extern int build_index_c(VectorIndexPtr& index_handler, float* vector_list, int64_t* ids, int dim, int size);
++
++typedef int (*add_index_ptr)(VectorIndexPtr& index_handler, float* vector, int64_t* ids, int dim, int size);
++extern int add_index_c(VectorIndexPtr& index_handler, float* vector, int64_t* ids, int dim, int size);
++
++typedef int (*get_index_number_ptr)(VectorIndexPtr& index_handler, int64_t &size);
++extern int get_index_number_c(VectorIndexPtr& index_handler, int64_t &size);
++
++typedef int (*knn_search_ptr)(VectorIndexPtr& index_handler,float* query_vector, int dim, int64_t topk,
++                      const float*& dist, const int64_t*& ids, int64_t &result_size, int ef_search,
++                       void* invalid);
++extern int knn_search_c(VectorIndexPtr& index_handler,float* query_vector, int dim, int64_t topk,
++                      const float*& dist, const int64_t*& ids, int64_t &result_size, int ef_search,
++                      void* invalid = NULL);
++
++typedef int (*serialize_ptr)(VectorIndexPtr& index_handler, const std::string dir);          
++extern int serialize_c(VectorIndexPtr& index_handler, const std::string dir);
++
++typedef int (*deserialize_bin_ptr)(VectorIndexPtr& index_handler, const std::string dir);
++extern int deserialize_bin_c(VectorIndexPtr& index_handler, const std::string dir);
++ 
++typedef int (*delete_index_ptr)(VectorIndexPtr& index_handler);
++extern int delete_index_c(VectorIndexPtr& index_handler);
++
++typedef int (*fserialize_ptr)(VectorIndexPtr& index_handler, std::ostream& out_stream);
++extern int fserialize_c(VectorIndexPtr& index_handler, std::ostream& out_stream);
++
++typedef int (*fdeserialize_ptr)(VectorIndexPtr& index_handler, std::istream& in_stream);
++extern int fdeserialize_c(VectorIndexPtr& index_handler, std::istream& in_stream);
++
++
++
++
++#ifdef __cplusplus
++}  // extern "C"
++#endif
++} // namesapce obvectorlib
++
++#endif // OB_VSAG_LIB_H
+
diff --git a/rpm/devdeps-abseil-cpp-build.sh b/rpm/devdeps-abseil-cpp-build.sh
new file mode 100644
index 0000000..f6b3899
--- /dev/null
+++ b/rpm/devdeps-abseil-cpp-build.sh
@@ -0,0 +1,54 @@
+#!/bin/bash
+
+CUR_DIR=$(dirname $(readlink -f "$0"))
+ROOT_DIR=$CUR_DIR/..
+PROJECT_DIR=${1:-"$ROOT_DIR"}
+PROJECT_NAME=${2:-"devdeps-abseil-cpp"}
+VERSION=${3:-"20211102.0"}
+RELEASE=${4:-"1"}
+
+# Configure custom source file directory
+[ -n "$SOURCE_DIR" ] && mv $SOURCE_DIR/* $ROOT_DIR
+
+# check source code
+if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/abseil-cpp-$VERSION.*[tar|gz|bz2|xz|zip]$"` ]]; then
+    echo "Download source code"
+    wget https://github.com/abseil/abseil-cpp/archive/refs/tags/${VERSION}.tar.gz -O $ROOT_DIR/abseil-cpp-$VERSION.tar.gz --no-check-certificate
+fi
+
+ID=$(grep -Po '(?<=^ID=).*' /etc/os-release | tr -d '"')
+ 
+if [[ "${ID}"x == "alinux"x ]]; then
+    wget http://mirrors.aliyun.com/oceanbase/OceanBaseAlinux.repo -P /etc/yum.repos.d/
+    yum install -y obdevtools-gcc9-9.3.0
+    yum install -y obdevtools-cmake-3.22.1
+else
+    os_release=`grep -Po '(?<=release )\d' /etc/redhat-release`
+    arch=`uname -p`
+    dep_pkgs=(obdevtools-gcc9-9.3.0-52022092914.el obdevtools-cmake-3.22.1-22022100417.el)
+ 
+    target_dir_3rd=${PROJECT_DIR}/deps/3rd
+    pkg_dir=$target_dir_3rd/pkg
+    mkdir -p $pkg_dir
+    for dep_pkg in ${dep_pkgs[@]}
+    do
+        TEMP=$(mktemp -p "/" -u ".XXXX")
+        download_base_url="https://mirrors.aliyun.com/oceanbase/development-kit/el"
+        deps_url=${download_base_url}/${os_release}/${arch}
+        pkg=${dep_pkg}${os_release}.${arch}.rpm
+        wget $deps_url/$pkg -O $pkg_dir/$TEMP
+        if [[ $? == 0 ]]; then
+            mv -f $pkg_dir/$TEMP $pkg_dir/$pkg
+        fi
+        (cd / && rpm2cpio $pkg_dir/$pkg | cpio -di -u --quiet)
+    done
+fi
+
+export TOOLS_DIR=/usr/local/oceanbase/devtools
+export PATH=$TOOLS_DIR/bin:$PATH
+export LD_LIBRARY_PATH=$TOOLS_DIR/lib:$TOOLS_DIR/lib64:$LD_LIBRARY_PATH
+export CC=$TOOLS_DIR/bin/gcc
+export CXX=$TOOLS_DIR/bin/g++
+
+cd $CUR_DIR
+bash $CUR_DIR/rpmbuild.sh $PROJECT_DIR $PROJECT_NAME $VERSION $RELEASE
diff --git a/rpm/devdeps-abseil-cpp.spec b/rpm/devdeps-abseil-cpp.spec
new file mode 100644
index 0000000..ec23945
--- /dev/null
+++ b/rpm/devdeps-abseil-cpp.spec
@@ -0,0 +1,51 @@
+Name: devdeps-abseil-cpp
+Version: 20211102.0
+Release: %(echo $RELEASE)%{?dist}
+Summary: Abseil is an open-source collection of C++ code (compliant to C++14) designed to augment the C++ standard library.
+
+Group: oceanbase-devel/dependencies
+License: Apache 2.0
+URL: https://github.com/abseil/abseil-cpp
+
+%undefine _missing_build_ids_terminate_build
+%define _build_id_links compat
+# disable check-buildroot
+%define __arch_install_post %{nil}
+# support debuginfo package, to reduce runtime package size
+%define debug_package %{nil}
+
+%define _prefix /usr/local/oceanbase/deps/devel
+%define _src abseil-cpp-%{version}
+
+%description
+The s2geometry-0.10.0 version depends on Abseil.
+
+%install
+mkdir -p $RPM_BUILD_ROOT/%{_prefix}
+export CFLAGS="-D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -pie -fstack-protector-strong"
+export CXXFLAGS="-D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -pie -fstack-protector-strong"
+export LDFLAGS="-pie -z noexecstack -z now"
+CPU_CORES=`grep -c ^processor /proc/cpuinfo`
+ROOT_DIR=$OLDPWD/..
+
+cd $ROOT_DIR
+rm -rf %{_src}
+mkdir -p %{_src}
+tar zxf %{_src}.tar.gz --strip-components=1 -C %{_src}
+cd %{_src}
+mkdir build && cd build
+cmake .. -DCMAKE_INSTALL_PREFIX=${RPM_BUILD_ROOT}/%{_prefix} -DABSL_BUILD_TESTING=OFF -DABSL_USE_GOOGLETEST_HEAD=ON -DCMAKE_CXX_STANDARD=14 -DCMAKE_POSITION_INDEPENDENT_CODE=ON -DCMAKE_BUILD_TYPE=RelWithDebInfo
+make -j${CPU_CORES}
+make install
+
+%files 
+
+%defattr(-,root,root)
+%{_prefix}
+
+%post -p /sbin/ldconfig
+%postun -p /sbin/ldconfig
+
+%changelog
+* Thu Dec 19 2024 huaixin.lmy
+- version 20211102.0
\ No newline at end of file
diff --git a/rpm/devdeps-cos-c-sdk-build.sh b/rpm/devdeps-cos-c-sdk-build.sh
index fa82fab..45e9f4e 100644
--- a/rpm/devdeps-cos-c-sdk-build.sh
+++ b/rpm/devdeps-cos-c-sdk-build.sh
@@ -14,7 +14,7 @@ if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/cos-c-sdk-$VERSION.*[tar|gz|bz2|
     git clone https://github.com/tencentyun/cos-c-sdk-v5.git cos-c-sdk-$VERSION
     cd cos-c-sdk-$VERSION
     git checkout v$VERSION
-    git apply ../cos_diff.patch
+    git apply ../patch/cos_diff.patch
     cd $ROOT_DIR
     tar -zcvf cos-c-sdk-$VERSION.tar.gz cos-c-sdk-$VERSION
 fi
diff --git a/rpm/devdeps-fast-float-build.sh b/rpm/devdeps-fast-float-build.sh
new file mode 100644
index 0000000..332c9dd
--- /dev/null
+++ b/rpm/devdeps-fast-float-build.sh
@@ -0,0 +1,53 @@
+#!/bin/bash
+
+CUR_DIR=$(dirname $(readlink -f "$0"))
+ROOT_DIR=$CUR_DIR/../
+PROJECT_DIR=${1:-"$CUR_DIR"}
+PROJECT_NAME=${2:-"devdeps-fast-float"}
+VERSION=${3:-"6.1.3"}
+RELEASE=${4:-"1"}
+
+# check source code
+if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/fast_float-$VERSION.*[tar|gz|bz2|xz|zip]$"` ]]; then
+    echo "Download source code"
+    cd $ROOT_DIR
+    wget https://github.com/fastfloat/fast_float/archive/refs/tags/v6.1.3.tar.gz -O $ROOT_DIR/fast_float-$VERSION.tar.gz --no-check-certificate
+fi
+
+yum install openssl-devel -y
+yum install openssl-libs -y
+
+# prepare building environment
+ID=$(grep -Po '(?<=^ID=).*' /etc/os-release | tr -d '"')
+if [[ "${ID}"x == "alinux"x ]]; then
+    wget http://mirrors.aliyun.com/oceanbase/OceanBaseAlinux.repo -P /etc/yum.repos.d/
+    yum install obdevtools-gcc9-9.3.0 -y
+    yum install obdevtools-cmake-3.22.1 -y
+else
+    os_release=`grep -Po '(?<=release )\d' /etc/redhat-release`
+    arch=`uname -p`
+    dep_pkgs=(obdevtools-gcc9-9.3.0-72024081318.el obdevtools-cmake-3.22.1-22022100417.el)
+    target_dir_3rd=${PROJECT_DIR}/deps/3rd
+    pkg_dir=$target_dir_3rd/pkg
+    mkdir -p $pkg_dir
+    for dep_pkg in ${dep_pkgs[@]}
+    do
+        TEMP=$(mktemp -p "/" -u ".XXXX")
+        download_base_url="https://mirrors.aliyun.com/oceanbase/development-kit/el"
+        deps_url=${download_base_url}/${os_release}/${arch}
+        pkg=${dep_pkg}${os_release}.${arch}.rpm
+        echo "start to download pkg from "$deps_url
+        wget $deps_url/$pkg -O $pkg_dir/$TEMP
+        if [[ $? == 0 ]]; then
+            mv -f $pkg_dir/$TEMP $pkg_dir/$pkg
+        fi
+        # rpm -ivh --force $pkg_dir/$pkg
+        (cd / && rpm2cpio $pkg_dir/$pkg | cpio -di -u --quiet)
+    done
+fi
+
+export PATH=/usr/local/oceanbase/devtools/bin:$PATH
+export LD_LIBRARY_PATH=/usr/local/oceanbase/devtools/lib:/usr/local/oceanbase/devtools/lib64:$LD_LIBRARY_PATH
+
+cd $CUR_DIR
+bash $CUR_DIR/rpmbuild.sh $PROJECT_DIR $PROJECT_NAME $VERSION $RELEASE
diff --git a/rpm/devdeps-fast-float.spec b/rpm/devdeps-fast-float.spec
new file mode 100644
index 0000000..5f44abf
--- /dev/null
+++ b/rpm/devdeps-fast-float.spec
@@ -0,0 +1,48 @@
+Name: devdeps-fast-float		
+Version: 6.1.3
+Release: %(echo $RELEASE)%{?dist}
+Summary: This is the repository for fast and exact implementation of the C++ from_chars functions for number types.
+
+Group: oceanbase-devel/dependencies
+License: Apache-2.0 License or MIT License or BSL-1.0 license
+URL: https://github.com/fastfloat/fast_float
+
+%undefine _missing_build_ids_terminate_build
+%define _build_id_links compat
+
+# disable check-buildroot
+%define __arch_install_post %{nil}
+
+%define _prefix /usr/local/oceanbase/deps/devel
+%define _src fast_float-%{version}
+
+%description
+The fast_float library provides fast header-only implementations for the C++ from_chars functions
+for float and double types as well as integer types.
+
+%install
+mkdir -p $RPM_BUILD_ROOT/%{_prefix}
+cd $OLDPWD/../;
+rm -rf %{_src}
+tar xvf %{_src}.tar.gz
+cd %{_src}
+mkdir -p cmake/build
+cd cmake/build
+cmake ../.. -DFASTFLOAT_TEST=ON \
+		-DCMAKE_BUILD_TYPE=RelWithDebInfo \
+		-DCMAKE_INSTALL_PREFIX=${RPM_BUILD_ROOT}/%{_prefix} \
+		-DBUILD_SHARED_LIBS=OFF
+CPU_CORES=`grep -c ^processor /proc/cpuinfo`
+make -j${CPU_CORES} install
+
+%files
+
+%defattr(-,root,root)
+%{_prefix}
+
+%post -p /sbin/ldconfig
+%postun -p /sbin/ldconfig
+
+%changelog
+* Mon Nov 4 2024 oceanbase
+- add spec of fast-float
diff --git a/rpm/devdeps-grpc-build.sh b/rpm/devdeps-grpc-build.sh
index ec8a85f..3d5c5cd 100644
--- a/rpm/devdeps-grpc-build.sh
+++ b/rpm/devdeps-grpc-build.sh
@@ -4,14 +4,14 @@ CUR_DIR=$(dirname $(readlink -f "$0"))
 ROOT_DIR=$CUR_DIR/../
 PROJECT_DIR=${1:-"$CUR_DIR"}
 PROJECT_NAME=${2:-"devdeps-grpc"}
-VERSION=${3:-"1.20.1"}
+VERSION=${3:-"1.46.7"}
 RELEASE=${4:-"1"}
 
 # check source code
 if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/grpc-$VERSION.*[tar|gz|bz2|xz|zip]$"` ]]; then
     echo "Download source code"
     cd $ROOT_DIR
-    git clone https://github.com/grpc/grpc.git -b v1.20.1 --depth 1 grpc-$VERSION
+    git clone https://github.com/grpc/grpc.git -b v1.46.7 --depth 1 grpc-$VERSION
     cd grpc-$VERSION
     git submodule update --init --recursive
     cd $ROOT_DIR
@@ -21,5 +21,40 @@ fi
 # prepare building environment
 # please prepare environment yourself if the following solution does not work for you.
 
+ID=$(grep -Po '(?<=^ID=).*' /etc/os-release | tr -d '"')
+
+if [[ "${ID}"x == "alinux"x ]]; then
+    wget http://mirrors.aliyun.com/oceanbase/OceanBaseAlinux.repo -P /etc/yum.repos.d/
+    yum install obdevtools-cmake-3.22.1 -y
+    yum install obdevtools-gcc9-9.3.0 -y
+else
+    RELEASE_ID=$(grep -Po '(?<=release )\d' /etc/redhat-release)
+    arch=`uname -p`
+    target_dir_3rd=${PROJECT_DIR}/deps/3rd
+    pkg_dir=$target_dir_3rd/pkg
+    mkdir -p $pkg_dir
+
+    dep_pkgs=(obdevtools-gcc9-9.3.0-72024081318.el obdevtools-cmake-3.22.1-22022100417.el)
+    download_base_url="https://mirrors.aliyun.com/oceanbase/development-kit/el"
+
+    for dep_pkg in ${dep_pkgs[@]}
+    do
+        TEMP=$(mktemp -p "/" -u ".XXXX")
+        deps_url=${download_base_url}/${RELEASE_ID}/${arch}
+        pkg=${dep_pkg}${RELEASE_ID}.${arch}.rpm
+        wget $deps_url/$pkg -O $pkg_dir/$TEMP
+        if [[ $? == 0 ]]; then
+            mv -f $pkg_dir/$TEMP $pkg_dir/$pkg
+        fi
+        (cd / && rpm2cpio $pkg_dir/$pkg | cpio -di -u --quiet)
+    done
+fi
+
+export PATH=/usr/local/oceanbase/devtools/bin:$PATH
+export LD_LIBRARY_PATH=/usr/local/oceanbase/devtools/lib:/usr/local/oceanbase/devtools/lib64:$LD_LIBRARY_PATH
+
+ln -sf /usr/local/oceanbase/devtools/bin/g++  /usr/bin/c++
+ln -sf /usr/local/oceanbase/devtools/bin/gcc  /usr/bin/cc
+
 cd $CUR_DIR
-bash $CUR_DIR/rpmbuild.sh $PROJECT_DIR $PROJECT_NAME $VERSION $RELEASE
\ No newline at end of file
+bash $CUR_DIR/rpmbuild.sh $PROJECT_DIR $PROJECT_NAME $VERSION $RELEASE
diff --git a/rpm/devdeps-grpc.spec b/rpm/devdeps-grpc.spec
index 5ad7e60..dd55158 100644
--- a/rpm/devdeps-grpc.spec
+++ b/rpm/devdeps-grpc.spec
@@ -1,5 +1,5 @@
 Name: devdeps-grpc
-Version: 1.20.1
+Version: 1.46.7
 Release: %(echo $RELEASE)%{?dist}
 Url: https://github.com/grpc/grpc
 Summary: gRPC is a modern, open source, high-performance remote procedure call (RPC) framework that can run anywhere.
@@ -35,8 +35,8 @@ cd %{_src}
 # # This means you will need to have external copies of these libraries available on your system.
 # # ref: https://github.com/grpc/grpc/blob/v1.21.0/test/distrib/cpp/run_distrib_test_cmake.sh
 
-export CFLAGS=-D_GLIBCXX_USE_CXX11_ABI=0
-export CXXFLAGS=-D_GLIBCXX_USE_CXX11_ABI=0
+export CFLAGS="-fPIC -D_GLIBCXX_USE_CXX11_ABI=0 -z noexecstack -z now -pie -fstack-protector-strong"
+export CXXFLAGS="-fPIC  -D_GLIBCXX_USE_CXX11_ABI=0 -z noexecstack -z now -pie -fstack-protector-strong"
 
 # Install c-ares
 cd third_party/cares/cares
@@ -63,13 +63,14 @@ rm -rf third_party/protobuf  # wipe out to prevent influencing the grpc build
 mkdir -p cmake/build
 cd cmake/build
 cmake ../.. -DgRPC_INSTALL=ON                \
-              -DgRPC_BUILD_TESTS=OFF           \
-              -DgRPC_PROTOBUF_PROVIDER=package  \
-              -DgRPC_ZLIB_PROVIDER=package      \
-              -DgRPC_CARES_PROVIDER=package     \
-              -DgRPC_SSL_PROVIDER=package       \
-              -DCMAKE_INSTALL_PREFIX=${RPM_BUILD_ROOT}/%{_prefix} \
-              -DBUILD_SHARED_LIBS=OFF
+            -DCMAKE_BUILD_TYPE=RelWithDebInfo	\
+            -DgRPC_BUILD_TESTS=OFF           \
+            -DgRPC_PROTOBUF_PROVIDER=package  \
+            -DgRPC_ZLIB_PROVIDER=package      \
+            -DgRPC_CARES_PROVIDER=package     \
+            -DgRPC_SSL_PROVIDER=package       \
+            -DCMAKE_INSTALL_PREFIX=${RPM_BUILD_ROOT}/%{_prefix} \
+            -DBUILD_SHARED_LIBS=OFF
 CPU_CORES=`grep -c ^processor /proc/cpuinfo`
 make -j${CPU_CORES} install
 
@@ -83,4 +84,4 @@ make -j${CPU_CORES} install
 
 %changelog
 * Mon Apr 12 2021 oceanbase
-- add spec of grpc
\ No newline at end of file
+- add spec of grpc
diff --git a/rpm/devdeps-hdfs-sdk-build.sh b/rpm/devdeps-hdfs-sdk-build.sh
new file mode 100644
index 0000000..8cdbed4
--- /dev/null
+++ b/rpm/devdeps-hdfs-sdk-build.sh
@@ -0,0 +1,120 @@
+#!/bin/bash
+
+CUR_DIR=$(dirname $(readlink -f "$0"))
+ROOT_DIR=$CUR_DIR/..
+PROJECT_DIR=${1:-"$ROOT_DIR"}
+PROJECT_NAME=${2:-"devdeps-hdfs-sdk"}
+VERSION=${3:-"3.3.6"}
+RELEASE=${4:-"1"}
+
+# Configure custom source file directory
+[ -n "$SOURCE_DIR" ] && mv $SOURCE_DIR/* $ROOT_DIR
+
+# check source code of hadoop
+if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/apache-hadoop-$VERSION.*[tar|gz|bz2|xz|zip]$"` ]]; then
+    echo "Download apache-hadoop source code"
+    wget https://github.com/apache/hadoop/archive/refs/tags/rel/release-$VERSION.tar.gz \
+    -O ${ROOT_DIR}/apache-hadoop-${VERSION}.tar.gz --no-check-certificate
+fi
+
+arch=$(uname -p)
+
+# download cmake for compiling hadoop
+CMAKE_VERSION="3.22.1"
+if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/cmake.*[tar|gz|bz2|xz|zip]$"` ]]; then
+    echo "Download cmake source code"
+    # https://github.com/Kitware/CMake/releases/download/v3.22.1/cmake-3.22.1.tar.gz
+    wget https://cmake.org/files/v3.22/cmake-${CMAKE_VERSION}.tar.gz \
+    -O ${ROOT_DIR}/cmake.tar.gz --no-check-certificate
+fi
+
+# Download jdk 1.8
+# jdk file is downloaded from "https://adoptium.net/zh-CN/temurin/releases/"
+JDK_VERSION="jdk8u432-b06"
+if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/OpenJDK8U-jdk.*[tar|gz|bz2|xz|zip]$"` ]]; then
+    echo "Download jdk 1.8 with arch $arch"
+    if [[ "$arch" == "x86_64" ]]; then
+        wget https://github.com/adoptium/temurin8-binaries/releases/download/${JDK_VERSION}/OpenJDK8U-jdk_x64_linux_hotspot_8u432b06.tar.gz \
+        -O ${ROOT_DIR}/OpenJDK8U-jdk-${arch}.tar.gz --no-check-certificate
+    elif [[ "$arch" == "aarch64" ]]; then
+        wget https://github.com/adoptium/temurin8-binaries/releases/download/${JDK_VERSION}/OpenJDK8U-jdk_aarch64_linux_hotspot_8u432b06.tar.gz \
+        -O ${ROOT_DIR}/OpenJDK8U-jdk-${arch}.tar.gz --no-check-certificate
+    else
+        echo "invalid arch: $arch to setup jdk 8" && exit 1
+    fi
+fi
+
+# Download maven
+MAVEN_VERSION="3.9.8"
+if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/apache-maven.*[tar|gz|bz2|xz|zip]$"` ]]; then
+    echo "Download apache-maven-3.9.8 source code"
+    wget https://dlcdn.apache.org/maven/maven-3/${MAVEN_VERSION}/binaries/apache-maven-${MAVEN_VERSION}-bin.tar.gz \
+    -O ${ROOT_DIR}/apache-maven.tar.gz --no-check-certificate
+fi
+
+# Download protobuf
+PROTOBUF_VERSION="3.7.1"
+if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/protobuf.*[tar|gz|bz2|xz|zip]$"` ]]; then
+    echo "Download protobuf source code"
+    wget https://github.com/protocolbuffers/protobuf/archive/refs/tags/v${PROTOBUF_VERSION}.tar.gz \
+    -O ${ROOT_DIR}/protobuf.tar.gz --no-check-certificate
+fi
+
+# Download texinfo
+TEXINFO_VERSION="7.1"
+if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/texinfo.*[tar|gz|bz2|xz|zip]$"` ]]; then
+    echo "download texinfo module"
+    wget https://ftp.gnu.org/gnu/texinfo/texinfo-${TEXINFO_VERSION}.tar.gz \
+    -O ${ROOT_DIR}/texinfo.tar.gz --no-check-certificate
+fi
+
+# Download gsasl
+GSASL_VERSION="2.2.1"
+if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/gsasl.*[tar|gz|bz2|xz|zip]$"` ]]; then
+    echo "install gsasl module"
+    wget https://ftp.gnu.org/gnu/gsasl/gsasl-${GSASL_VERSION}.tar.gz \
+    -O ${ROOT_DIR}/gsasl.tar.gz --no-check-certificate
+fi
+
+# build gcc9 dependencies
+ID=$(grep -Po '(?<=^ID=).*' /etc/os-release | tr -d '"')
+yum install -y libtool
+yum install -y m4
+
+if [[ "${ID}"x == "alinux"x ]]; then
+    wget http://mirrors.aliyun.com/oceanbase/OceanBaseAlinux.repo -P /etc/yum.repos.d/
+    yum install obdevtools-gcc9-9.3.0 -y
+else
+    target_dir_3rd=${PROJECT_DIR}/deps/3rd
+    pkg_dir=$target_dir_3rd/pkg
+    mkdir -p $pkg_dir
+    RELEASE_ID=$(grep -Po '(?<=release )\d' /etc/redhat-release)
+    dep_pkgs=(obdevtools-gcc9-9.3.0-72024081318.el)
+    download_base_url="https://mirrors.aliyun.com/oceanbase/development-kit/el"
+ 
+    for dep_pkg in ${dep_pkgs[@]}
+    do
+        TEMP=$(mktemp -p "/" -u ".XXXX")
+        deps_url=${download_base_url}/${RELEASE_ID}/${arch}
+        pkg=${dep_pkg}${RELEASE_ID}.${arch}.rpm
+        wget $deps_url/$pkg -O $pkg_dir/$TEMP
+        if [[ $? == 0 ]]; then
+            mv -f $pkg_dir/$TEMP $pkg_dir/$pkg
+        fi
+        (cd / && rpm2cpio $pkg_dir/$pkg | cpio -di -u --quiet)
+    done
+    # wget http://mirrors.aliyun.com/oceanbase/OceanBase.repo -P /etc/yum.repos.d/
+    # yum install obdevtools-gcc9-9.3.0 -y
+fi
+
+export TOOLS_DIR=/usr/local/oceanbase/devtools
+export PATH=$TOOLS_DIR/bin:$PATH
+export LD_LIBRARY_PATH=$TOOLS_DIR/lib:$TOOLS_DIR/lib64:$LD_LIBRARY_PATH
+export CC=$TOOLS_DIR/bin/gcc
+export CXX=$TOOLS_DIR/bin/g++
+
+ln -sf $TOOLS_DIR/bin/g++  /usr/bin/c++
+ln -sf $TOOLS_DIR/bin/gcc  /usr/bin/cc
+
+cd $CUR_DIR
+bash $CUR_DIR/rpmbuild.sh $PROJECT_DIR $PROJECT_NAME $VERSION $RELEASE
diff --git a/rpm/devdeps-hdfs-sdk.spec b/rpm/devdeps-hdfs-sdk.spec
new file mode 100644
index 0000000..8672066
--- /dev/null
+++ b/rpm/devdeps-hdfs-sdk.spec
@@ -0,0 +1,143 @@
+Name: devdeps-hdfs-sdk
+Version: 3.3.6
+Release: %(echo $RELEASE)%{?dist}
+Summary: This is the repository for accessing files on hdfs store 
+License: https://github.com/apache/hadoop/blob/trunk/LICENSE.txt
+AutoReqProv:no
+%undefine _missing_build_ids_terminate_build
+%define _build_id_links compat
+# disable check-buildroot
+%define __arch_install_post %{nil}
+# support debuginfo package, to reduce runtime package size
+%define debug_package %{nil}
+
+%define _prefix /usr/local/oceanbase/deps/devel
+%define _cmake_src cmake
+%define _OpenJDK8U_src OpenJDK8U-jdk
+%define _apache_maven_src apache-maven
+%define _protobuf_src protobuf
+%define _texinfo_src texinfo
+%define _gsasl_src gsasl
+%define _src_path hadoop-rel-release-3.3.6
+%define _src apache-hadoop-3.3.6
+%define _product_prefix hdfs
+
+# prepare env variables for compiling hdfspp 
+%define _compiled_prefix hadoop-hdfs-project/hadoop-hdfs-native-client
+%define _compiled_libs %_compiled_prefix/target/native/target/usr/local/lib
+%define _header_files %_compiled_prefix/src/main/native/libhdfs/include/hdfs
+
+%description
+This is the repository for accessing files on hdfs store 
+
+%install
+# create dirs
+mkdir -p $RPM_BUILD_ROOT/%{_prefix}/lib
+mkdir -p $RPM_BUILD_ROOT/%{_prefix}/include/%{_product_prefix}
+CPU_CORES=`grep -c ^processor /proc/cpuinfo`
+ROOT_DIR=$OLDPWD/..
+
+# install cmake
+cd $ROOT_DIR
+rm -rf %{_cmake_src}
+mkdir -p %{_cmake_src}
+tar zxf %{_cmake_src}.tar.gz --strip-components=1 -C %{_cmake_src}
+cd %{_cmake_src}
+./bootstrap --prefix=$ROOT_DIR/%{_cmake_src} -- -DCMAKE_USE_OPENSSL=ON
+make -j${CPU_CORES}
+make install
+
+# install jdk
+arch=`uname -p`
+cd $ROOT_DIR
+rm -rf %{_OpenJDK8U_src}
+mkdir -p %{_OpenJDK8U_src}
+tar zxf %{_OpenJDK8U_src}-$arch.tar.gz --strip-components 1 -C %{_OpenJDK8U_src}
+
+# install maven
+rm -rf %{_apache_maven_src}
+mkdir -p %{_apache_maven_src}
+tar zxf %{_apache_maven_src}.tar.gz --strip-components 1 -C %{_apache_maven_src}
+cp -rf patch/settings.xml %{_apache_maven_src}/conf
+
+# install protobuf
+rm -rf %{_protobuf_src}
+mkdir -p %{_protobuf_src}
+tar zxf %{_protobuf_src}.tar.gz --strip-components 1 -C %{_protobuf_src}
+cd %{_protobuf_src}
+./autogen.sh
+./configure --prefix=$ROOT_DIR/%{_protobuf_src} CFLAGS="-g -O2 -fPIC" CXXFLAGS="-g -O2 -fPIC"
+make -j${CPU_CORES}
+make install
+
+# install texinfo
+cd $ROOT_DIR
+rm -rf %{_texinfo_src}
+mkdir -p %{_texinfo_src}
+tar zxf %{_texinfo_src}.tar.gz --strip-components 1 -C %{_texinfo_src}
+cd %{_texinfo_src}
+./configure --prefix=$ROOT_DIR/%{_texinfo_src}
+make -j${CPU_CORES}
+make install
+
+# install gsasl
+cd $ROOT_DIR
+rm -rf %{_gsasl_src}
+mkdir -p %{_gsasl_src}
+tar zxf %{_gsasl_src}.tar.gz --strip-components 1 -C %{_gsasl_src}
+cd %{_gsasl_src}
+./configure --prefix=$ROOT_DIR/%{_gsasl_src} CFLAGS="-g -O2 -fPIC" CXXFLAGS="-g -O2 -fPIC"
+make -j${CPU_CORES}
+make install
+
+cd $ROOT_DIR
+
+# compile and install `hdfspp`, note: use gcc and g++ as same as the compiler of observer
+export JAVA_HOME=$ROOT_DIR/%{_OpenJDK8U_src}
+export PROTOBUF_HOME=$ROOT_DIR/%{_protobuf_src}
+export GSASL_HOME=$ROOT_DIR/%{_gsasl_src}
+export TEXINFO_HOME=$ROOT_DIR/%{_texinfo_src}
+export MAVEN_HOME=$ROOT_DIR/%{_apache_maven_src}
+export PATH=$GSASL_HOME/bin:$TEXINFO_HOME/bin:$JAVA_HOME/bin:$MAVEN_HOME/bin:$ROOT_DIR/%{_cmake_src}/bin:$PROTOBUF_HOME/bin:$PATH;
+
+rm -rf %{_src_path}
+tar xf %{_src}.tar.gz
+cd %{_src_path}
+git init
+git apply --whitespace=fix ../patch/hdfs.patch
+
+mvn -pl :hadoop-hdfs-native-client -Pnative compile -Dnative_make_args="copy_hadoop_files"
+
+## install protobuf
+# echo "Copy libprotobuf.a from ${PROTOBUF_HOME}/lib/libprotobuf.a to $RPM_BUILD_ROOT/%{_prefix}/lib/"
+# cp ${PROTOBUF_HOME}/lib/libprotobuf.a $RPM_BUILD_ROOT/%{_prefix}/lib/
+## install gsasl
+# echo "Copy libgsasl.a from ${GSASL_HOME}/lib/libgsasl.a to $RPM_BUILD_ROOT/%{_prefix}/lib/"
+# cp ${GSASL_HOME}/lib/libgsasl.a $RPM_BUILD_ROOT/%{_prefix}/lib/
+## install hdfspp
+cp -P %_compiled_libs/libhdfs.so* $RPM_BUILD_ROOT/%{_prefix}/lib/
+mkdir -p $RPM_BUILD_ROOT/%{_prefix}/include/%{_product_prefix}
+cp -r %_header_files/* $RPM_BUILD_ROOT/%{_prefix}/include/%{_product_prefix}/
+
+# install mocked libjvm.so
+# cp $LIBMOCKJVM $RPM_BUILD_ROOT/%{_prefix}/lib/libjvm.so
+
+## copy jni header files
+cp $ROOT_DIR/%{_OpenJDK8U_src}/include/jni.h $RPM_BUILD_ROOT/%{_prefix}/include/
+cp $ROOT_DIR/%{_OpenJDK8U_src}/include/linux/* $RPM_BUILD_ROOT/%{_prefix}/include/
+
+%files 
+
+%defattr(-,root,root)
+
+%{_prefix}
+%exclude %dir %{_prefix}
+%exclude %dir %{_prefix}/include
+%exclude %dir %{_prefix}/lib
+
+%post -p /sbin/ldconfig
+%postun -p /sbin/ldconfig
+
+%changelog
+* Thu Nov 21 2024 huaixin.lmy
+- version 3.3.6
diff --git a/rpm/devdeps-icu-build.sh b/rpm/devdeps-icu-build.sh
index a5bc7e5..e6686ef 100644
--- a/rpm/devdeps-icu-build.sh
+++ b/rpm/devdeps-icu-build.sh
@@ -13,32 +13,38 @@ if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/icu-release-69-1.*[tar|gz|bz2|xz
     wget https://github.com/unicode-org/icu/archive/refs/tags/release-69-1.tar.gz -O $ROOT_DIR/icu-release-69-1.tar.gz --no-check-certificate
 fi
 
-os_release=`grep -Po '(?<=release )\d' /etc/redhat-release`
-arch=`uname -p`
-
 # build dependencies
-dep_pkgs=(obdevtools-gcc9-9.3.0-52022092914.el obdevtools-cmake-3.22.1-22022100417.el)
-
-target_dir_3rd=${PROJECT_DIR}/deps/3rd
-pkg_dir=$target_dir_3rd/pkg
-mkdir -p $pkg_dir
-for dep_pkg in ${dep_pkgs[@]}
-do
-    TEMP=$(mktemp -p "/" -u ".XXXX")
-    download_base_url="https://mirrors.aliyun.com/oceanbase/development-kit/el"
-    deps_url=${download_base_url}/${os_release}/${arch}
-    pkg=${dep_pkg}${os_release}.${arch}.rpm
-    echo "start to download pkg from "$deps_url
-    wget $deps_url/$pkg -O $pkg_dir/$TEMP
-    if [[ $? == 0 ]]; then
-        mv -f $pkg_dir/$TEMP $pkg_dir/$pkg
-    fi
-    (cd $target_dir_3rd && rpm2cpio $pkg_dir/$pkg | cpio -di -u --quiet)
-done
-
-export TOOLS_DIR=$target_dir_3rd/usr/local/oceanbase/devtools
-export DEP_DIR=$target_dir_3rd/usr/local/oceanbase/deps/devel
+ID=$(grep -Po '(?<=^ID=).*' /etc/os-release | tr -d '"')
+
+if [[ "${ID}"x == "alinux"x ]]; then
+    wget http://mirrors.aliyun.com/oceanbase/OceanBaseAlinux.repo -P /etc/yum.repos.d/
+    yum install obdevtools-gcc9-9.3.0 -y
+    yum install obdevtools-cmake-3.22.1 -y
+else
+    os_release=`grep -Po '(?<=release )\d' /etc/redhat-release`
+    arch=`uname -p`
+    dep_pkgs=(obdevtools-gcc9-9.3.0-52022092914.el obdevtools-cmake-3.22.1-22022100417.el)
+    target_dir_3rd=${PROJECT_DIR}/deps/3rd
+    pkg_dir=$target_dir_3rd/pkg
+    mkdir -p $pkg_dir
+    for dep_pkg in ${dep_pkgs[@]}
+    do
+        TEMP=$(mktemp -p "/" -u ".XXXX")
+        download_base_url="https://mirrors.aliyun.com/oceanbase/development-kit/el"
+        deps_url=${download_base_url}/${os_release}/${arch}
+        pkg=${dep_pkg}${os_release}.${arch}.rpm
+        echo "start to download pkg from "$deps_url
+        wget $deps_url/$pkg -O $pkg_dir/$TEMP
+        if [[ $? == 0 ]]; then
+            mv -f $pkg_dir/$TEMP $pkg_dir/$pkg
+        fi
+        (cd / && rpm2cpio $pkg_dir/$pkg | cpio -di -u --quiet)
+    done
+fi
 
+export TOOLS_DIR=/usr/local/oceanbase/devtools
+export DEP_DIR=/usr/local/oceanbase/deps/devel
 
 cd $CUR_DIR
 bash $CUR_DIR/rpmbuild.sh $PROJECT_DIR $PROJECT_NAME $VERSION $RELEASE
+
diff --git a/rpm/devdeps-icu.spec b/rpm/devdeps-icu.spec
index 4d78a99..609ccdd 100644
--- a/rpm/devdeps-icu.spec
+++ b/rpm/devdeps-icu.spec
@@ -39,6 +39,10 @@ mkdir -p ${build_dir}
 export PATH=$TOOLS_DIR/bin/:$PATH
 export CC=$TOOLS_DIR/bin/gcc
 export CXX=$TOOLS_DIR/bin/g++
+
+export CFLAGS="-fPIC -D_GLIBCXX_USE_CXX11_ABI=0 -z noexecstack -z now -pie -fstack-protector-strong"
+export CXXFLAGS="-fPIC  -D_GLIBCXX_USE_CXX11_ABI=0 -z noexecstack -z now -pie -fstack-protector-strong"
+
 cd ${build_dir}
 cmake .. -DICU_VERSION_DIR=icu4c -DCMAKE_INSTALL_PREFIX=${tmp_install_dir} -DCMAKE_POSITION_INDEPENDENT_CODE=ON -DCMAKE_BUILD_TYPE=Release
 CPU_CORES=`grep -c ^processor /proc/cpuinfo`
@@ -48,9 +52,6 @@ make install
 # install files
 cp -r ${tmp_install_dir}/lib/*.a $RPM_BUILD_ROOT/%{_prefix}/lib
 cp -r ${tmp_install_dir}/include/* $RPM_BUILD_ROOT/%{_prefix}/include/%{_product_prefix}
-mkdir -p $RPM_BUILD_ROOT/%{_prefix}/include/%{_product_prefix}/i18n/unicode
-cp ../icu4c/source/i18n/unicode/*.h $RPM_BUILD_ROOT/%{_prefix}/include/%{_product_prefix}/i18n/unicode/
-
 
 # package infomation
 %files 
diff --git a/rpm/devdeps-ncurses-static-build.sh b/rpm/devdeps-ncurses-static-build.sh
index 4a6a51a..d14bea1 100644
--- a/rpm/devdeps-ncurses-static-build.sh
+++ b/rpm/devdeps-ncurses-static-build.sh
@@ -1,16 +1,17 @@
 #!/bin/bash
 
 CUR_DIR=$(dirname $(readlink -f "$0"))
-ROOT_DIR=$CUR_DIR/../
+ROOT_DIR=$CUR_DIR/..
 PROJECT_DIR=${1:-"$CUR_DIR"}
 PROJECT_NAME=${2:-"devdeps-ncurses-static"}
-VERSION=${3:-"6.2"}
+VERSION=${3:-"6.4"}
 RELEASE=${4:-"1"}
 
 # check source code
 if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/ncurses-$VERSION.*[tar|gz|bz2|xz|zip]$"` ]]; then
     echo "Download source code"
-    wget --no-check-certificate https://invisible-mirror.net/archives/ncurses/ncurses-6.2.tar.gz -P $ROOT_DIR
+    # wget --no-check-certificate https://invisible-mirror.net/archives/ncurses/ncurses-6.2.tar.gz -P $ROOT_DIR
+    wget --no-check-certificate https://ftp.gnu.org/gnu/ncurses/ncurses-6.4.tar.gz -P $ROOT_DIR
 fi
 
 cd $CUR_DIR
diff --git a/rpm/devdeps-ncurses-static.spec b/rpm/devdeps-ncurses-static.spec
index 5724be3..6fd2d40 100644
--- a/rpm/devdeps-ncurses-static.spec
+++ b/rpm/devdeps-ncurses-static.spec
@@ -1,5 +1,5 @@
 Name: devdeps-ncurses-static
-Version: 6.2
+Version: 6.4
 Release: %(echo $RELEASE)%{?dist}
 Url: http://invisible-island.net/ncurses/ncurses.html
 Summary: Static libraries for the ncurses library
diff --git a/rpm/devdeps-prometheus-cpp-build.sh b/rpm/devdeps-prometheus-cpp-build.sh
index cbde523..0f85fe9 100644
--- a/rpm/devdeps-prometheus-cpp-build.sh
+++ b/rpm/devdeps-prometheus-cpp-build.sh
@@ -4,30 +4,57 @@ CUR_DIR=$(dirname $(readlink -f "$0"))
 ROOT_DIR=$CUR_DIR/../
 PROJECT_DIR=${1:-"$CUR_DIR"}
 PROJECT_NAME=${2:-"devdeps-prometheus-cpp"}
-VERSION=${3:-"0.8.0"}
+VERSION=${3:-"1.1.0"}
 RELEASE=${4:-"1"}
 
 # check source code
 if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/prometheus-cpp-$VERSION.*[tar|gz|bz2|xz|zip]$"` ]]; then
     echo "Download source code"
     cd $ROOT_DIR
-    git clone https://github.com/jupp0r/prometheus-cpp.git -b v0.8.0 --depth 1 prometheus-cpp-$VERSION
+    git clone https://github.com/jupp0r/prometheus-cpp.git -b v${VERSION} --depth 1 prometheus-cpp-$VERSION
     cd prometheus-cpp-$VERSION
-    git submodule update --init
+    git submodule update --init --recursive --depth 1
     cd $ROOT_DIR
-    tar -zcvf prometheus-cpp-$VERSION.tar.gz prometheus-cpp-$VERSION
+    tar -zcf prometheus-cpp-$VERSION.tar.gz prometheus-cpp-$VERSION
 fi
 
 # prepare building environment
-# please prepare environment yourself if the following solution does not work for you.
-# Please use gcc5.2 or higher version
-OS_RELEASE=$(grep -Po '(?<=release )\d' /etc/redhat-release)
-if [[ x"$OS_RELEASE" == x"7" ]]; then
-    echo "Install gcc 8"
-    yum install centos-release-scl -y
-    yum install devtoolset-8 -y
-    source /opt/rh/devtoolset-8/enable
+ID=$(grep -Po '(?<=^ID=).*' /etc/os-release | tr -d '"')
+
+if [[ "${ID}"x == "alinux"x ]]; then
+    wget http://mirrors.aliyun.com/oceanbase/OceanBaseAlinux.repo -P /etc/yum.repos.d/
+    yum install obdevtools-cmake-3.22.1 -y
+    yum install obdevtools-gcc9-9.3.0 -y
+else
+    wget http://mirrors.aliyun.com/oceanbase/OceanBase.repo -P /etc/yum.repos.d/
+    yum install obdevtools-gcc9-9.3.0 -y
+    RELEASE_ID=$(grep -Po '(?<=release )\d' /etc/redhat-release)
+    arch=`uname -p`
+    target_dir_3rd=${PROJECT_DIR}/deps/3rd
+    pkg_dir=$target_dir_3rd/pkg
+    mkdir -p $pkg_dir
+
+    dep_pkgs=(obdevtools-cmake-3.22.1-22022100417.el)
+    download_base_url="https://mirrors.aliyun.com/oceanbase/development-kit/el"
+
+    for dep_pkg in ${dep_pkgs[@]}
+    do
+        TEMP=$(mktemp -p "/" -u ".XXXX")
+        deps_url=${download_base_url}/${RELEASE_ID}/${arch}
+        pkg=${dep_pkg}${RELEASE_ID}.${arch}.rpm
+        wget $deps_url/$pkg -O $pkg_dir/$TEMP
+        if [[ $? == 0 ]]; then
+            mv -f $pkg_dir/$TEMP $pkg_dir/$pkg
+        fi
+        (cd / && rpm2cpio $pkg_dir/$pkg | cpio -di -u --quiet)
+    done
 fi
 
+export PATH=/usr/local/oceanbase/devtools/bin:$PATH
+export LD_LIBRARY_PATH=/usr/local/oceanbase/devtools/lib:/usr/local/oceanbase/devtools/lib64:$LD_LIBRARY_PATH
+
+export CC=/usr/local/oceanbase/devtools/bin/gcc 
+export CXX=/usr/local/oceanbase/devtools/bin/g++
+
 cd $CUR_DIR
 bash $CUR_DIR/rpmbuild.sh $PROJECT_DIR $PROJECT_NAME $VERSION $RELEASE
\ No newline at end of file
diff --git a/rpm/devdeps-prometheus-cpp.spec b/rpm/devdeps-prometheus-cpp.spec
index 2afb765..aee4a22 100644
--- a/rpm/devdeps-prometheus-cpp.spec
+++ b/rpm/devdeps-prometheus-cpp.spec
@@ -1,5 +1,5 @@
 Name: devdeps-prometheus-cpp
-Version: 0.8.0
+Version: 1.1.0
 Release: %(echo $RELEASE)%{?dist}
 Url: https://github.com/jupp0r/prometheus-cpp
 Summary: This library implements the Prometheus Data Model to enable Metrics-Driven Development for C++ services.
@@ -24,18 +24,19 @@ This library aims to enable Metrics-Driven Development for C++ services.
 It implements the Prometheus Data Model, a powerful abstraction on which to collect and expose metrics.
 
 %install
-
 mkdir -p $RPM_BUILD_ROOT/%{_prefix}
 cd $OLDPWD/../;
 rm -rf %{_src}
-tar xvf %{_src}.tar.gz
+tar xf %{_src}.tar.gz
 cd %{_src}
 mkdir _build
 cd _build
+
 # Please use gcc5.2 or higher version
-export CFLAGS=-D_GLIBCXX_USE_CXX11_ABI=0
-export CXXFLAGS=-D_GLIBCXX_USE_CXX11_ABI=0
-cmake .. -DCMAKE_INSTALL_PREFIX=%{_prefix} -DBUILD_SHARED_LIBS=OFF
+export CFLAGS="-D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -fstack-protector-strong"
+export CXXFLAGS="-D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -fstack-protector-strong"
+export LDFLAGS="-pie -z noexecstack -z now"
+cmake .. -DCMAKE_INSTALL_PREFIX=%{_prefix} -DBUILD_SHARED_LIBS=OFF -DENABLE_TESTING=OFF
 CPU_CORES=`grep -c ^processor /proc/cpuinfo`
 make -j${CPU_CORES};
 make DESTDIR=$RPM_BUILD_ROOT install
diff --git a/rpm/devdeps-relaxed-rapidjson-build.sh b/rpm/devdeps-relaxed-rapidjson-build.sh
index 11d585e..1ba25f1 100644
--- a/rpm/devdeps-relaxed-rapidjson-build.sh
+++ b/rpm/devdeps-relaxed-rapidjson-build.sh
@@ -28,7 +28,7 @@ cd $ROOT_DIR
 unzip $ROOT_DIR/devdeps-relaxed-rapidjson-$VERSION.zip
 cd $ROOT_DIR/rapidjson-27c3a8dc0e2c9218fe94986d249a12b5ed838f1d
 echo "move patch file here "$(pwd)
-mv $ROOT_DIR/devdeps-relaxed-rapidjson.diff .
+mv $ROOT_DIR/patch/devdeps-relaxed-rapidjson.diff .
 patch -p1 < devdeps-relaxed-rapidjson.diff
 
 cd rpm
diff --git a/rpm/devdeps-s2geometry-build.sh b/rpm/devdeps-s2geometry-build.sh
index ad7720c..79f9a1c 100644
--- a/rpm/devdeps-s2geometry-build.sh
+++ b/rpm/devdeps-s2geometry-build.sh
@@ -4,7 +4,7 @@ CUR_DIR=$(dirname $(readlink -f "$0"))
 ROOT_DIR=$CUR_DIR/..
 PROJECT_DIR=${1:-"$ROOT_DIR"}
 PROJECT_NAME=${2:-"devdeps-s2geometry"}
-VERSION=${3:-"0.9.0"}
+VERSION=${3:-"0.10.0"}
 RELEASE=${4:-"1"}
 
 # check source code
@@ -13,31 +13,44 @@ if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/s2geometry-$VERSION.*[tar|gz|bz2
     wget https://github.com/google/s2geometry/archive/refs/tags/v${VERSION}.tar.gz -O $ROOT_DIR/s2geometry-$VERSION.tar.gz
 fi
 
-os_release=`grep -Po '(?<=release )\d' /etc/redhat-release`
 arch=`uname -p`
+# prepare building environment
+ID=$(grep -Po '(?<=^ID=).*' /etc/os-release | tr -d '"')
+ 
+if [[ "${ID}"x == "alinux"x ]]; then
+    wget http://mirrors.aliyun.com/oceanbase/OceanBaseAlinux.repo -P /etc/yum.repos.d/
+    yum install -y obdevtools-gcc9-9.3.0
+    yum install -y obdevtools-cmake-3.22.1
+    yum install -y devdeps-openssl-static-1.1.1u
+    yum install -y devdeps-abseil-cpp-20211102.0
+else
+    os_release=`grep -Po '(?<=release )\d' /etc/redhat-release`
+    dep_pkgs=(obdevtools-gcc9-9.3.0-52022092914.el obdevtools-cmake-3.22.1-22022100417.el devdeps-openssl-static-1.1.1u-22023100710.el devdeps-abseil-cpp-20211102.0-62024122014.el)
+ 
+    target_dir_3rd=${PROJECT_DIR}/deps/3rd
+    pkg_dir=$target_dir_3rd/pkg
+    mkdir -p $pkg_dir
+    for dep_pkg in ${dep_pkgs[@]}
+    do
+        TEMP=$(mktemp -p "/" -u ".XXXX")
+        download_base_url="https://mirrors.aliyun.com/oceanbase/development-kit/el"
+        deps_url=${download_base_url}/${os_release}/${arch}
+        pkg=${dep_pkg}${os_release}.${arch}.rpm
+        wget $deps_url/$pkg -O $pkg_dir/$TEMP
+        if [[ $? == 0 ]]; then
+            mv -f $pkg_dir/$TEMP $pkg_dir/$pkg
+        fi
+        (cd / && rpm2cpio $pkg_dir/$pkg | cpio -di -u --quiet)
+    done
+fi
 
-# build dependencies
-dep_pkgs=(obdevtools-gcc9-9.3.0-52022092914.el obdevtools-cmake-3.22.1-22022100417.el devdeps-openssl-static-1.0.1e-12022100422.el)
-
-target_dir_3rd=${PROJECT_DIR}/deps/3rd
-pkg_dir=$target_dir_3rd/pkg
-mkdir -p $pkg_dir
-for dep_pkg in ${dep_pkgs[@]}
-do
-    TEMP=$(mktemp -p "/" -u ".XXXX")
-    download_base_url="https://mirrors.aliyun.com/oceanbase/development-kit/el"
-    deps_url=${download_base_url}/${os_release}/${arch}
-    pkg=${dep_pkg}${os_release}.${arch}.rpm
-    wget $deps_url/$pkg -O $pkg_dir/$TEMP
-    if [[ $? == 0 ]]; then
-        mv -f $pkg_dir/$TEMP $pkg_dir/$pkg
-    fi
-    (cd $target_dir_3rd && rpm2cpio $pkg_dir/$pkg | cpio -di -u --quiet)
-done
-
-export TOOLS_DIR=$target_dir_3rd/usr/local/oceanbase/devtools
-export DEP_DIR=$target_dir_3rd/usr/local/oceanbase/deps/devel
+export DEP_DIR=/usr/local/oceanbase/deps/devel
+export PATH=/usr/local/oceanbase/devtools/bin:$PATH
+export ABSL_DIR=$DEP_DIR/lib64/cmake/absl/
+export LD_LIBRARY_PATH=/usr/local/oceanbase/devtools/lib:/usr/local/oceanbase/devtools/lib64:$LD_LIBRARY_PATH
 
+ln -sf /usr/local/oceanbase/devtools/bin/g++  /usr/bin/c++
+ln -sf /usr/local/oceanbase/devtools/bin/gcc  /usr/bin/cc
 
 cd $CUR_DIR
 bash $CUR_DIR/rpmbuild.sh $PROJECT_DIR $PROJECT_NAME $VERSION $RELEASE
diff --git a/rpm/devdeps-s2geometry.spec b/rpm/devdeps-s2geometry.spec
index 1278948..305aca2 100644
--- a/rpm/devdeps-s2geometry.spec
+++ b/rpm/devdeps-s2geometry.spec
@@ -1,15 +1,15 @@
 Name: devdeps-s2geometry
-Version: 0.9.0
+Version: 0.10.0
 Release: %(echo $RELEASE)%{?dist}
 Summary: This is a package for manipulating geometric shapes.
 Group: alibaba/application
-License: Apache2.0
+License: Apache 2.0
 AutoReqProv:no
 %undefine _missing_build_ids_terminate_build
 %define _build_id_links compat
 %define _prefix /usr/local/oceanbase/deps/devel
 %define _product_prefix s2
-%define _src s2geometry-0.9.0
+%define _src s2geometry-%{version}
 
 
 %description
@@ -24,7 +24,7 @@ This makes it especially suitable for working with geographic data.
 %install
 # create dirs
 mkdir -p $RPM_BUILD_ROOT/%{_prefix}/include/%{_product_prefix}
-mkdir -p $RPM_BUILD_ROOT/%{_prefix}/lib
+mkdir -p $RPM_BUILD_ROOT/%{_prefix}/lib64
 cd $OLDPWD/../;
 rm -rf %{_src}
 tar xf %{_src}.tar.gz
@@ -37,35 +37,21 @@ rm -rf ${build_dir}
 mkdir -p ${tmp_install_dir}
 mkdir -p ${build_dir}
 
-# disable compiling python interface
-sed -i '/find_package(SWIG)/d' ${source_dir}/CMakeLists.txt
-sed -i '/find_package(PythonInterp)/d' ${source_dir}/CMakeLists.txt
-sed -i '/find_package(PythonLibs)/d' ${source_dir}/CMakeLists.txt
-
-# disable compiling test file
-sed -i '/add_library(s2testing STATIC/d' ${source_dir}/CMakeLists.txt
-sed -i '/s2builderutil_testing.cc/d' ${source_dir}/CMakeLists.txt
-sed -i '/s2shapeutil_testing.cc/d' ${source_dir}/CMakeLists.txt
-sed -i '/s2testing.cc/d' ${source_dir}/CMakeLists.txt
-sed -i 's/install(TARGETS s2 s2testing DESTINATION lib)/install(TARGETS s2 DESTINATION lib)/' ${source_dir}/CMakeLists.txt
-
-# fix uint64 error in aarch64: https://github.com/google/s2geometry/pull/166
-cp ${source_dir}/../s2geometry-0.9.0-uint64.patch ${source_dir}
-patch -p0 < s2geometry-0.9.0-uint64.patch
-
 # compile and install
-export PATH=$TOOLS_DIR/bin/:$PATH
-export CC=$TOOLS_DIR/bin/gcc
-export CXX=$TOOLS_DIR/bin/g++
+export CFLAGS="-D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -pie -fstack-protector-strong"
+export CXXFLAGS="-D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -pie -fstack-protector-strong"
+export LDFLAGS="-pie -z noexecstack -z now"
+
 cd ${build_dir}
-OPENSSL_ROOT_DIR=$DEP_DIR cmake .. -DCMAKE_INSTALL_PREFIX=${tmp_install_dir} -DBUILD_SHARED_LIBS=OFF -DBUILD_EXAMPLES=OFF -DCMAKE_POSITION_INDEPENDENT_CODE=ON -DCMAKE_BUILD_TYPE=Release
+OPENSSL_ROOT_DIR=$DEP_DIR
+cmake .. -DCMAKE_INSTALL_PREFIX=${tmp_install_dir} -DCMAKE_PREFIX_PATH=$DEP_DIR -DCMAKE_CXX_STANDARD=14 -DCMAKE_CXX_STANDARD_REQUIRED=ON -DBUILD_SHARED_LIBS=OFF -DBUILD_EXAMPLES=OFF -DCMAKE_POSITION_INDEPENDENT_CODE=ON -DCMAKE_BUILD_TYPE=Release
 CPU_CORES=`grep -c ^processor /proc/cpuinfo`
 make -j${CPU_CORES}
 make install
 
 # install files
 cp -r ${tmp_install_dir}/include/s2/* $RPM_BUILD_ROOT/%{_prefix}/include/%{_product_prefix}
-cp -r ${tmp_install_dir}/lib/* $RPM_BUILD_ROOT/%{_prefix}/lib
+cp -r ${tmp_install_dir}/lib64/* $RPM_BUILD_ROOT/%{_prefix}/lib64/
 
 # package infomation
 %files 
@@ -78,5 +64,7 @@ cp -r ${tmp_install_dir}/lib/* $RPM_BUILD_ROOT/%{_prefix}/lib
 %postun -p /sbin/ldconfig
 
 %changelog
+* Thu Dec 19 2024 huaixin.lmy
+- version 0.10.0
 * Mon Mar 09 2022 xuhao.yf
-- version 0.9.0
\ No newline at end of file
+- version 0.9.0
diff --git a/rpm/devdeps-vsag-build.sh b/rpm/devdeps-vsag-build.sh
new file mode 100644
index 0000000..1a19bb7
--- /dev/null
+++ b/rpm/devdeps-vsag-build.sh
@@ -0,0 +1,59 @@
+#!/bin/bash
+ 
+CUR_DIR=$(dirname $(readlink -f "$0"))
+ROOT_DIR=$CUR_DIR/../
+PROJECT_DIR=${1:-"$CUR_DIR"}
+PROJECT_NAME=${2:-"devdeps-vsag"}
+VERSION=${3:-"1.0.0"}
+RELEASE=${4:-"1"}
+ 
+# check source code
+if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/vsag-$VERSION.*[tar|gz|bz2|xz|zip]$"` ]]; then
+    echo "Download vsag source code"
+    for cnt in {1..6}
+    do
+        echo "Download source code with retry cnt = "$cnt
+        wget --no-check-certificate http://github.com/alipay/vsag/archive/refs/tags/v0.11.11.tar.gz -O $ROOT_DIR/vsag-$VERSION.tar.gz
+	if [[ $? == 0 ]];then
+            break
+        fi
+    done
+fi
+
+# prepare building environment
+ID=$(grep -Po '(?<=^ID=).*' /etc/os-release | tr -d '"')
+
+if [[ "${ID}"x == "alinux"x ]]; then
+    wget http://mirrors.aliyun.com/oceanbase/OceanBaseAlinux.repo -P /etc/yum.repos.d/
+    sed -i '6s/enabled=1/enabled=0/' /etc/yum.repos.d/OceanBaseAlinux.repo
+    yum install obdevtools-gcc9-9.3.0 -y
+    yum install obdevtools-cmake-3.22.1 -y
+else
+    os_release=`grep -Po '(?<=release )\d' /etc/redhat-release`
+    arch=`uname -p`
+    dep_pkgs=(obdevtools-gcc9-9.3.0-72024081318.el obdevtools-cmake-3.22.1-22022100417.el)
+    target_dir_3rd=${PROJECT_DIR}/deps/3rd
+    pkg_dir=$target_dir_3rd/pkg
+    mkdir -p $pkg_dir
+    for dep_pkg in ${dep_pkgs[@]}
+    do
+        TEMP=$(mktemp -p "/" -u ".XXXX")
+        download_base_url="https://mirrors.aliyun.com/oceanbase/development-kit/el"
+        deps_url=${download_base_url}/${os_release}/${arch}
+        pkg=${dep_pkg}${os_release}.${arch}.rpm
+        echo "start to download pkg from "$deps_url
+        wget $deps_url/$pkg -O $pkg_dir/$TEMP
+        if [[ $? == 0 ]]; then
+            mv -f $pkg_dir/$TEMP $pkg_dir/$pkg
+        fi
+        # rpm -ivh --force $pkg_dir/$pkg
+        (cd / && rpm2cpio $pkg_dir/$pkg | cpio -di -u --quiet)
+    done
+fi
+ 
+export PATH=/usr/local/oceanbase/devtools/bin:$PATH
+export LD_LIBRARY_PATH=/usr/local/oceanbase/devtools/lib:/usr/local/oceanbase/devtools/lib64:$LD_LIBRARY_PATH
+ 
+cd $CUR_DIR
+bash $CUR_DIR/rpmbuild.sh $PROJECT_DIR $PROJECT_NAME $VERSION $RELEASE
+ 
diff --git a/rpm/devdeps-vsag.spec b/rpm/devdeps-vsag.spec
new file mode 100644
index 0000000..1b27dda
--- /dev/null
+++ b/rpm/devdeps-vsag.spec
@@ -0,0 +1,80 @@
+Name: devdeps-vsag	
+Version: %(echo $VERSION)	
+Release: %(echo $RELEASE)%{?dist}
+Summary: VSAG is a vector indexing library used for similarity search.
+ 
+License: GPLv2 or Apache 2.0
+URL: https://github.com/alipay/vsag
+ 
+%undefine _missing_build_ids_terminate_build
+%define _prefix /usr/local/oceanbase/deps/devel
+%define _vsag_src vsag-%{version}
+%define debug_package %{nil}
+%define _default_version_src vsag-1.0.0
+%define _gcc_path /usr/local/oceanbase/devtools/bin
+%define _install_prefix ./install
+ 
+%description
+A library that provides fast ann query algorithm.
+ 
+%install
+mkdir -p %{buildroot}/%{_prefix}
+cd $OLDPWD/../
+rm -rf %{_vsag_src}
+tar xf %{_vsag_src}.tar.gz
+mv vsag-0.11.11 %{_default_version_src}
+cd %{_default_version_src}
+ 
+export CC=/usr/local/oceanbase/devtools/bin/gcc
+export CXX=/usr/local/oceanbase/devtools/bin/g++
+export FC=/usr/local/oceanbase/devtools/bin/gfortran
+
+export CFLAGS="-fPIC -D_GLIBCXX_USE_CXX11_ABI=0 -z noexecstack -z now -pie -fstack-protector-strong"
+export CXXFLAGS="-fPIC  -D_GLIBCXX_USE_CXX11_ABI=0 -z noexecstack -z now -pie -fstack-protector-strong"
+ 
+git init
+git apply ../patch/vsag.patch
+cmake .
+ 
+CPU_CORES=`grep -c ^processor /proc/cpuinfo`
+make  -j${CPU_CORES}
+ 
+mkdir -p %{buildroot}/%{_prefix}/lib/vsag_lib
+mkdir -p %{buildroot}/%{_prefix}/include/vsag
+cp ./ob_vsag_lib.h %{buildroot}/%{_prefix}/include/vsag
+cp ./ob_vsag_lib_c.h %{buildroot}/%{_prefix}/include/vsag
+cp ./include/vsag/logger.h %{buildroot}/%{_prefix}/include/vsag
+cp ./include/vsag/allocator.h %{buildroot}/%{_prefix}/include/vsag
+cp /usr/local/oceanbase/devtools/lib64/libgfortran.so.5 %{buildroot}/%{_prefix}/lib/vsag_lib/libgfortran.so
+cp /usr/local/oceanbase/devtools/lib64/libgfortran.a %{buildroot}/%{_prefix}/lib/vsag_lib/libgfortran_static.a
+cp /usr/local/oceanbase/devtools/lib64/libgomp.a %{buildroot}/%{_prefix}/lib/vsag_lib/libgomp_static.a
+cp /usr/local/oceanbase/devtools/lib64/libgomp.so %{buildroot}/%{_prefix}/lib/vsag_lib/libgomp.so
+cp ./libob_vsag_static.a %{buildroot}/%{_prefix}/lib/vsag_lib
+cp ./libob_vsag.so %{buildroot}/%{_prefix}/lib/vsag_lib
+cp ./src/libvsag.so %{buildroot}/%{_prefix}/lib/vsag_lib
+cp ./src/libvsag_static.a %{buildroot}/%{_prefix}/lib/vsag_lib
+cp ./src/simd/libsimd.a %{buildroot}/%{_prefix}/lib/vsag_lib
+cp ./_deps/cpuinfo-build/libcpuinfo.a %{buildroot}/%{_prefix}/lib/vsag_lib
+cp ./libdiskann.a %{buildroot}/%{_prefix}/lib/vsag_lib
+cp ./openblas/install/lib/libopenblas.a %{buildroot}/%{_prefix}/lib/vsag_lib
+#cp ./_deps/roaringbitmap-build/src/libroaring.a %{buildroot}/%{_prefix}/lib/vsag_lib
+ 
+arch=$(uname -p)
+if [ "$arch" = "x86_64" ]; then
+    cp /usr/local/oceanbase/devtools/lib64/libquadmath.so %{buildroot}/%{_prefix}/lib/vsag_lib
+    cp /usr/local/oceanbase/devtools/lib64/libquadmath.a %{buildroot}/%{_prefix}/lib/vsag_lib/libquadmath_static.a
+fi
+ 
+%files
+ 
+%defattr(-,root,root)
+ 
+%{_prefix}
+ 
+%post -p /sbin/ldconfig
+%postun -p /sbin/ldconfig
+ 
+%changelog
+* Thu Sep 26 2024 oceanbase
+- vsag-1.0.0
+ 
diff --git a/rpm/obdevtools-gcc-build.sh b/rpm/obdevtools-gcc-build.sh
index 3ab6f80..e668edf 100644
--- a/rpm/obdevtools-gcc-build.sh
+++ b/rpm/obdevtools-gcc-build.sh
@@ -1,20 +1,28 @@
 #!/bin/bash
 
 CUR_DIR=$(dirname $(readlink -f "$0"))
-ROOT_DIR=$CUR_DIR/../
+ROOT_DIR=$CUR_DIR/..
 PROJECT_DIR=${1:-"$CUR_DIR"}
 PROJECT_NAME=${2:-"obdevtools-gcc"}
-VERSION=${3:-"5.2.0"}
+VERSION=${3:-"12.3.0"}
 RELEASE=${4:-"1"}
 
 # check source code
 if [[ -z `find $ROOT_DIR -maxdepth 1 -regex ".*/gcc-$VERSION.*[tar|gz|bz2|xz|zip]$"` ]]; then
     echo "Download source code"
-    wget https://mirrors.aliyun.com/gnu/gcc/gcc-5.2.0/gcc-5.2.0.tar.gz -P $ROOT_DIR
-    wget --no-check-certificate https://gcc.gnu.org/pub/gcc/infrastructure/mpc-0.8.1.tar.gz -P $ROOT_DIR
-    wget --no-check-certificate https://gcc.gnu.org/pub/gcc/infrastructure/mpfr-2.4.2.tar.bz2 -P $ROOT_DIR
-    wget --no-check-certificate https://gcc.gnu.org/pub/gcc/infrastructure/isl-0.14.tar.bz2 -P $ROOT_DIR
-    wget --no-check-certificate https://gcc.gnu.org/pub/gcc/infrastructure/gmp-4.3.2.tar.bz2 -P $ROOT_DIR
+    wget https://mirrors.aliyun.com/gnu/gcc/gcc-${VERSION}/gcc-${VERSION}.tar.gz -P $ROOT_DIR
+    wget --no-check-certificate https://gcc.gnu.org/pub/gcc/infrastructure/mpc-1.2.1.tar.gz -P $ROOT_DIR
+    wget --no-check-certificate https://gcc.gnu.org/pub/gcc/infrastructure/mpfr-4.1.0.tar.bz2 -P $ROOT_DIR
+    wget --no-check-certificate https://gcc.gnu.org/pub/gcc/infrastructure/isl-0.24.tar.bz2 -P $ROOT_DIR
+    wget --no-check-certificate https://gcc.gnu.org/pub/gcc/infrastructure/gmp-6.2.1.tar.bz2 -P $ROOT_DIR
+fi
+
+# prepare building environment
+OS_RELEASE=$(grep -Po '(?<=release )\d' /etc/redhat-release)
+if [[ x"$OS_RELEASE" == x"7" ]]; then
+    yum install -y centos-release-scl
+    yum install -y devtoolset-8-gcc devtoolset-8-gcc-c++
+    source /opt/rh/devtoolset-8/enable
 fi
 
 cd $CUR_DIR
diff --git a/rpm/obdevtools-gcc.spec b/rpm/obdevtools-gcc.spec
index c64cf81..de7fda8 100644
--- a/rpm/obdevtools-gcc.spec
+++ b/rpm/obdevtools-gcc.spec
@@ -1,5 +1,5 @@
 Name: obdevtools-gcc
-Version: 5.2.0
+Version: 12.3.0
 Release: %(echo $RELEASE)%{?dist}
 
 Summary: The GNU Compiler Collection
@@ -18,15 +18,15 @@ AutoReqProv:no
 
 %define _prefix /usr/local/oceanbase/devtools/
 %define _gcc_src gcc-%{version}
-%define _mpc_src mpc-0.8.1
-%define _mpfr_src mpfr-2.4.2
-%define _isl_src isl-0.14
-%define _gmp_src gmp-4.3.2
+%define _mpc_src mpc-1.2.1
+%define _mpfr_src mpfr-4.1.0
+%define _isl_src isl-0.24
+%define _gmp_src gmp-6.2.1
 
 %description
 The GNU Compiler Collection includes front ends for C, C++, Fortran, Lto as well as libraries for these languages (libstdc++,...). GCC was originally written as the compiler for the GNU operating system. The GNU system was developed to be 100% free software, free in the sense that it respects the user's freedom.
 
-%debug_package
+%define debug_package %{nil}
 
 %install
 cd $OLDPWD/../; 
@@ -43,7 +43,9 @@ tar -xf %{_isl_src}.tar.bz2
 mv %{_isl_src} %{_gcc_src}/isl
 
 cd %{_gcc_src}
-./configure --enable-bootstrap --enable-languages=c,c++,fortran,lto --prefix=${RPM_BUILD_ROOT}/%{_prefix} --enable-shared --enable-threads=posix --enable-checking=release --disable-multilib --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --with-default-libstdcxx-abi=gcc4-compatible --enable-plugin --enable-initfini-array --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux
+arch=$(uname -m)
+gcc_build=${arch}-redhat-linux
+./configure --enable-bootstrap --enable-languages=c,c++,fortran,lto --prefix=${RPM_BUILD_ROOT}/%{_prefix} --enable-shared --enable-threads=posix --enable-checking=release --disable-multilib --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --with-default-libstdcxx-abi=gcc4-compatible --enable-plugin --enable-initfini-array --enable-gnu-indirect-function --build=${gcc_build}
 
 CPU_CORES=`grep -c ^processor /proc/cpuinfo`
 make -j${CPU_CORES};
